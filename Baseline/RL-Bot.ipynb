{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:57:10.788192Z",
     "start_time": "2019-04-05T18:57:10.784685Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.model import *\n",
    "from src.preprocessing import * \n",
    "from src.utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:34:33.418691Z",
     "start_time": "2019-04-05T18:34:33.409021Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(loadFilename,params):\n",
    "    # hyperparameters\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "    PAD_token = 0  # Used for padding short sentences\n",
    "    SOS_token = 1  # Start-of-sentence token\n",
    "    EOS_token = 2  # End-of-sentence token\n",
    "    MAX_LENGTH = params['MAX_LENGTH']  # Maximum sentence length to consider\n",
    "    MIN_COUNT = params['MIN_COUNT']    # Minimum word count threshold for trimming\n",
    "    save_dir = params['save_dir']\n",
    "    emo_dict = params['emo_dict']\n",
    "    model_name = params['model_name']\n",
    "    corpus_name = params['corpus_name']\n",
    "    attn_model = params['attn_model']\n",
    "    voc = Voc(corpus_name,max_length=MAX_LENGTH,min_count=MIN_COUNT)\n",
    "    hidden_size = params['hidden_size']\n",
    "    encoder_n_layers = params['encoder_n_layers']\n",
    "    decoder_n_layers = params['decoder_n_layers']\n",
    "    dropout = params['dropout']\n",
    "    batch_size = params['batch_size']\n",
    "    # number of emotion\n",
    "    num_emotions = params['num_emotions']\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename, map_location='cpu')\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "    emotion_words = checkpoint['external_memory']\n",
    "\n",
    "    print('Building encoder and decoder ...')\n",
    "    # Initialize word embeddings\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    emotion_embedding = nn.Embedding(num_emotions, hidden_size)\n",
    "    if loadFilename:\n",
    "        embedding.load_state_dict(embedding_sd)\n",
    "    # Initialize encoder & decoder models\n",
    "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "    decoder = LuongAttnDecoderRNN(attn_model, embedding, emotion_embedding, hidden_size,\n",
    "                                  voc.num_words, emotion_words, decoder_n_layers, dropout)\n",
    "    if loadFilename:\n",
    "        encoder.load_state_dict(encoder_sd)\n",
    "        decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "    # Use appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    return encoder,decoder,voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:34:33.428188Z",
     "start_time": "2019-04-05T18:34:33.420777Z"
    }
   },
   "outputs": [],
   "source": [
    "emo_dict ={ 0: 'no emotion', 1: 'anger', 2: 'disgust', \n",
    "            3: 'fear', 4: 'happiness', \n",
    "            5: 'sadness', 6: 'surprise'}\n",
    "params = {\n",
    "    'MAX_LENGTH':20,\n",
    "    'MIN_COUNT':1,\n",
    "    'save_dir':os.path.join('data','save'),\n",
    "    'emo_dict':emo_dict,\n",
    "    'model_name':'emotion_model1',\n",
    "    'corpus_name':'daily_dialogue',\n",
    "    'attn_model':'dot',\n",
    "    'hidden_size':500,\n",
    "    'encoder_n_layers':2,\n",
    "    'decoder_n_layers':2,\n",
    "    'num_emotions':7,\n",
    "    'dropout':0.1,\n",
    "    'batch_size':64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:34:33.862066Z",
     "start_time": "2019-04-05T18:34:33.430554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n"
     ]
    }
   ],
   "source": [
    "encoder1,decoder1,voc1 = load_model('data/pre_trained/fully_work_ECM_4000checkpoint_500_hidden.tar',params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:34:34.264595Z",
     "start_time": "2019-04-05T18:34:33.864308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n"
     ]
    }
   ],
   "source": [
    "encoder2,decoder2,voc2 = load_model('data/pre_trained/5000_checkpoint.tar',params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam search for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:42:20.913085Z",
     "start_time": "2019-04-05T18:42:20.900009Z"
    }
   },
   "outputs": [],
   "source": [
    "class BeamSearchDecoder(nn.Module):\n",
    "    '''\n",
    "    Beam search decode\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder,num_word,device):\n",
    "        '''\n",
    "\n",
    "        :param encoder: torch nn\n",
    "        :param decoder: torch nn\n",
    "        :param num_word: int vocabulary size\n",
    "        '''\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.num_word = num_word\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_seq,target_emotions,input_length, max_length):\n",
    "        SOS_token = 1\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=self.device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=self.device, dtype=torch.long)\n",
    "        all_words_order = torch.zeros((1,self.num_word),device=self.device,dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=self.device)\n",
    "        all_scores_array = torch.zeros((1,self.num_word),device=self.device,dtype=torch.float)\n",
    "        # Set initial context value,last_rnn_output, internal_memory\n",
    "        context_input = torch.ones(1,self.decoder.hidden_size,dtype=torch.float)\n",
    "        context_input = context_input.to(self.device)\n",
    "        # last_rnn_output = torch.FloatTensor(hidden_size)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden,target_emotions,context_input = self.decoder(\n",
    "                decoder_input,target_emotions, decoder_hidden,\n",
    "                context_input, encoder_outputs\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            decoder_input_order = torch.argsort(decoder_output,dim=1,descending=True)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            all_scores_array = torch.cat((all_scores_array,decoder_output),dim = 0)\n",
    "            all_words_order = torch.cat((all_words_order,decoder_input_order), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        sequences = self.beam_search(all_scores_array,3)\n",
    "        return sequences\n",
    "    def beam_search(self,array,k):\n",
    "        array = array.tolist()\n",
    "        sequences = [[list(), 1.0]]\n",
    "        # walk over each step in sequence\n",
    "        for row in array:\n",
    "            all_candidates = list()\n",
    "            # expand each current candidate\n",
    "            for i in range(len(sequences)):\n",
    "                seq, score = sequences[i]\n",
    "                for j in range(len(row)):\n",
    "                    candidate = [seq + [j], score * -np.log(row[j] + 1e-8)]\n",
    "                    all_candidates.append(candidate)\n",
    "            # order all candidates by score\n",
    "            ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "            # select k best\n",
    "            sequences = ordered[:k]\n",
    "        return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T19:00:08.670602Z",
     "start_time": "2019-04-05T19:00:08.664573Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, emotions,device,max_length):\n",
    "    emotions = int(emotions)\n",
    "    emotions = torch.LongTensor([emotions])\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    emotions = emotions.to(device)\n",
    "    # indexes -> words \n",
    "    sequences = searcher(input_batch, emotions, lengths, max_length)\n",
    "    decoded_words = beam_decode(sequences, voc)\n",
    "    return decoded_words\n",
    "def beam_decode(sequences, voc):\n",
    "    for each in sequences:\n",
    "        score = each[-1]\n",
    "        for idxs in each:\n",
    "            return [[voc.index2word[idx] for idx in idxs[:-1]],score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T19:15:30.244415Z",
     "start_time": "2019-04-05T19:15:30.240138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "encoder1.eval()\n",
    "decoder1.eval()\n",
    "encoder2.eval()\n",
    "decoder2.eval()\n",
    "# Initialize search module\n",
    "searcher1 = BeamSearchDecoder(encoder1,decoder1,voc1.num_words,device)\n",
    "searcher2 = BeamSearchDecoder(encoder2,decoder2,voc2.num_words,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T19:15:31.631753Z",
     "start_time": "2019-04-05T19:15:31.626464Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_response(num_emotions,sentence,searcher,voc):\n",
    "    sentence = sentence.lower()\n",
    "    responses = []\n",
    "    for i in range(num_emotions):\n",
    "        responses.append(evaluate(searcher,voc,sentence,i,device,voc.max_length))\n",
    "    responses.sort(key = lambda x:x[1],reverse = True)\n",
    "    output_words =[]\n",
    "    for each in responses[0][0]:\n",
    "        if each == 'EOS':\n",
    "            return ' '.join(output_words)\n",
    "        elif each == 'PAD':\n",
    "            continue\n",
    "        else:\n",
    "            output_words.append(each)\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T19:15:32.614932Z",
     "start_time": "2019-04-05T19:15:32.611393Z"
    }
   },
   "outputs": [],
   "source": [
    "def play(num_emotions,sentence,searcher1,searcher2,voc1,voc2):\n",
    "    print('Human:',sentence)\n",
    "    while True:\n",
    "        sentence = get_response(num_emotions,sentence,searcher1,voc1)\n",
    "        print('Bot1:',sentence)\n",
    "        sentence = get_response(num_emotions,sentence,searcher2,voc2)\n",
    "        print('Bot2:',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T19:20:16.311588Z",
     "start_time": "2019-04-05T19:18:59.870749Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: where is your wife\n",
      "Bot1: she is in the bedroom .\n",
      "Bot2: what is her ?\n",
      "Bot1: she is expecting you don t .\n",
      "Bot2: well i wish you .\n",
      "Bot1: you too .\n",
      "Bot2: how s going to dance .\n",
      "Bot1: i ll say .\n",
      "Bot2: i d like any more questions .\n",
      "Bot1: give them in the line price .\n",
      "Bot2: the night is .\n",
      "Bot1: i have to get in the bedroom .\n",
      "Bot2: have you ever any pay ?\n",
      "Bot1: no . i have only one mid .\n",
      "Bot2: how about the pay or one ?\n",
      "Bot1: cash or cash please .\n",
      "Bot2: would you like to pay ?\n",
      "Bot1: i d be very pleased .\n",
      "Bot2: thank you very much .\n",
      "Bot1: you re welcome .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4fecb736ba64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'where is your wife'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearcher1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearcher2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-c017e8280149>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(num_emotions, sentence, searcher1, searcher2, voc1, voc2)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_emotions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearcher1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bot1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_emotions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearcher2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bot2:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2ad988f129a6>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(num_emotions, sentence, searcher, voc)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_emotions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-ee895c6b34e2>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(searcher, voc, sentence, emotions, device, max_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# indexes -> words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdecoded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PythonProject/Emotional Chatbot/Baseline/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, target_emotions, input_length, max_length)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# Return collections of word tokens and scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PythonProject/Emotional Chatbot/Baseline/src/model.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, array, k)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                     \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                     \u001b[0mall_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# order all candidates by score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_sentence = 'where is your wife'\n",
    "play(7,start_sentence,searcher1,searcher2,voc1,voc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
