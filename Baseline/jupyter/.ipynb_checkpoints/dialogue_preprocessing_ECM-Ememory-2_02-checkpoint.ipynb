{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:14.079320Z",
     "start_time": "2019-04-03T00:20:12.231583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from preprocessing_dailydialogue import *\n",
    "import pickle\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:14.085039Z",
     "start_time": "2019-04-03T00:20:14.081116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define constant\n",
    "# Default word tokens\n",
    "#\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "emo_dict = { 0: 'neutral', 1: 'joy', 2: 'anger', \n",
    "            3: 'sadness',4:'fear'}\n",
    "emo2idx = {value:key for key,value in emo_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this block if using daily dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data from pickle (No preprocessing required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('processed_train.pickle','rb') as f:\n",
    "    pairs = pickle.load(f)\n",
    "    pairs_emotion = pickle.load(f)\n",
    "    voc = pickle.load(f)\n",
    "#with open('processed_test.pickle','rb') as f:\n",
    "#    pairs_t = pickle.load(f)\n",
    "#    pairs_emotion_t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch = batch2TrainData(voc,list(range(1000)),pairs[-1000:],pairs_emotion[-1000:])\n",
    "pairs = pairs[:-1000]\n",
    "pairs_emotion = pairs_emotion[:-1000]\n",
    "test_pairs = pairs[-1000:]\n",
    "test_pairs_emotion = pairs_emotion[-1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14923, 14923)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs),len(pairs_emotion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.371122Z",
     "start_time": "2019-04-03T00:20:22.361048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[ 386,   85,   25,   16,  304],\n",
      "        [  85,   10,   12,   10,   12],\n",
      "        [2647,  445,   64,  188,   45],\n",
      "        [ 261, 2084,  489,  517,   14],\n",
      "        [2648,   13,   45,   15,   99],\n",
      "        [2649,   92,  342,    2,    2],\n",
      "        [ 108,  906,   66,    0,    0],\n",
      "        [   8,   15,    8,    0,    0],\n",
      "        [   2,    2,    2,    0,    0]])\n",
      "Input_emotion: tensor([0, 0, 1, 0, 2])\n",
      "lengths: tensor([9, 9, 9, 6, 6])\n",
      "target_variable: tensor([[ 134,   52,   32,   32,   32],\n",
      "        [ 818,   54,  656,   27,    2],\n",
      "        [  85,  190,    8,  834,    0],\n",
      "        [ 386,  321,   27,   13,    0],\n",
      "        [ 244,  366,  155,  261,    0],\n",
      "        [ 103,   13,  657, 1083,    0],\n",
      "        [  15,   77,    8,    8,    0],\n",
      "        [   2,  985,    2,    2,    0],\n",
      "        [   0,    8,    0,    0,    0],\n",
      "        [   0,    2,    0,    0,    0]])\n",
      "target_emotion: tensor([0, 0, 1, 0, 2])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 10\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(list(range(len(test_pairs)))) for _ in range(small_batch_size)],test_pairs,test_pairs_emotion)\n",
    "input_variable,input_emotion, lengths, target_variable,target_emotion, mask, max_target_len = batches\n",
    "#loss = evaluate_performance(input_variable,lengths,target_variable,target_emotion,mask,max_target_len,encoder,decoder)\n",
    "print(\"input_variable:\", input_variable)\n",
    "print('Input_emotion:',input_emotion)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print('target_emotion:',target_emotion)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)\n",
    "#print('Loss:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.384721Z",
     "start_time": "2019-04-03T00:20:22.372826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement attention inside ECM\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "# Luong attention layer\n",
    "'''\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   \n",
    "        '''\n",
    "print('Implement attention inside ECM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECM: Internal memory + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.394866Z",
     "start_time": "2019-04-03T00:20:22.386874Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ECMWrapper(nn.Module):\n",
    "    '''\n",
    "    Internal memory module\n",
    "    '''\n",
    "    def __init__(self,hidden_size,state_size,emo_size,num_emotion,embedding,emotion_embedding,gru,device):\n",
    "        '''\n",
    "        hidden_size: hidden input dimension\n",
    "        state_size: state vector size (input a word so hidden size)\n",
    "        emo_size: emotional embedding size (usually similar to hidden_size)\n",
    "        num_emotion: number of emotion categories\n",
    "        '''\n",
    "        super(ECMWrapper,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.emo_size = emo_size\n",
    "        self.num_emotion = num_emotion\n",
    "        self.device = device\n",
    "        # read gate dimensions (word_embedding + hidden_input + context_input)\n",
    "        self.read_g = nn.Linear(self.hidden_size + self.hidden_size + self.hidden_size,self.emo_size)\n",
    "        # write gate\n",
    "        self.write_g = nn.Linear(self.state_size, self.emo_size)\n",
    "        # GRU output input dimensions = state_last + context + emotion emb + internal memory\n",
    "        self.gru = gru\n",
    "        self.emotion_embedding = emotion_embedding\n",
    "        self.embedding = embedding\n",
    "        # attention layer\n",
    "        self.attn1 = nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        self.attn2 = nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        self.concat = nn.Linear(self.hidden_size, 1)\n",
    "    def forward(self,word_input,decoder_output,emotion_input,context_input,last_hidden,memory):\n",
    "        '''\n",
    "        Last hidden == prev_cell_state\n",
    "        last word embedding = word_input\n",
    "        last hidden input = h\n",
    "        last_rnn_output = logits before softmax\n",
    "        memory = encoder_outputs\n",
    "        '''\n",
    "        # get embedding of input word and emotion\n",
    "        if decoder_output is None:\n",
    "            decoder_output = torch.zeros(word_input.shape[1],self.hidden_size,dtype=torch.float,device = self.device)\n",
    "            decoder_output = decoder_output.unsqueeze(0)\n",
    "            context_input = self._compute_context(decoder_output,memory)\n",
    "        last_word_embedding = self.embedding(word_input)\n",
    "        read_inputs = torch.cat((last_word_embedding,decoder_output,context_input), dim = -1)\n",
    "        # compute read input\n",
    "        # write to emotion embedding\n",
    "        emotion_input = self._read_internal_memory(read_inputs,emotion_input)\n",
    "        # pass everything to GRU\n",
    "        # decoder_output: logits from last rnn unit\n",
    "        X = torch.cat([last_word_embedding,decoder_output, context_input, emotion_input], dim = -1)\n",
    "        rnn_output, hidden = self.gru(X,last_hidden)\n",
    "        # write input\n",
    "        # update states\n",
    "        # write to emotion embedding\n",
    "        new_M_emo = self._write_internal_memory(emotion_input,rnn_output) # new emotion_input\n",
    "        new_context = self._compute_context(rnn_output,memory)\n",
    "        return rnn_output, hidden, new_M_emo, new_context\n",
    "    def _compute_context(self,rnn_output,memory):\n",
    "        '''\n",
    "        Compute context\n",
    "        '''\n",
    "        rnn_output = rnn_output.unsqueeze(dim=-2).squeeze(0) # make shape (batch,1,hidden_size)\n",
    "        memory = memory.permute(1,0,2)\n",
    "        Wq = self.attn1(rnn_output)\n",
    "        Wm = self.attn2(memory)\n",
    "        concat = (Wq + Wm).tanh()\n",
    "        e = self.concat(concat).squeeze(2)\n",
    "        attn_score = torch.softmax(e,dim = 1).unsqueeze(1)\n",
    "        context = torch.bmm(attn_score,memory).squeeze(1)\n",
    "        return context.unsqueeze(0)\n",
    "    def _read_internal_memory(self,read_inputs,emotion_input):\n",
    "        \"\"\"\n",
    "        Read the internal memory\n",
    "            emotion_input: [batch_size, emo_hidden_size]\n",
    "            read_inputs: [batch_size, d] d= [last_word_embedding;decoder_output;context_input]\n",
    "        Returns:\n",
    "            output: [batch_size, emo__hidden_size]\n",
    "        \"\"\"\n",
    "        read_inputs = self.read_g(read_inputs)\n",
    "        M_read = torch.sigmoid(read_inputs)\n",
    "        return emotion_input * M_read\n",
    "    def _write_internal_memory(self,emotion_input,rnn_output):\n",
    "        \"\"\"\n",
    "        Write the internal memory\n",
    "            emotion_input: [batch_size, emo_hidden_size]\n",
    "            rnn_output: [batch_size, hidden_size]\n",
    "        Returns:\n",
    "            output: [batch_size, emo_hidden_size]\n",
    "        \"\"\"\n",
    "        M_write = torch.sigmoid(self.write_g(rnn_output))\n",
    "        return emotion_input * M_write\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.410809Z",
     "start_time": "2019-04-03T00:20:22.396793Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding,emotion_embedding, hidden_size, output_size,device,ememory=None, n_layers=1, dropout=0.1,num_emotions = 7,batch_size = 64):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_emotions = num_emotions\n",
    "        self.device = device\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        # define emotion embedding\n",
    "        self.emotion_embedding = emotion_embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        #self.emotion_embedding_dropout = nn.Dropout(dropout)\n",
    "        # dimension\n",
    "        self.gru = nn.GRU(hidden_size + hidden_size + hidden_size + hidden_size , hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        # using in Luong et al. attention mechanism.\n",
    "        self.internal_memory = ECMWrapper(hidden_size,hidden_size,\n",
    "                                          hidden_size,self.num_emotions,\n",
    "                                          self.embedding,self.emotion_embedding,self.gru,device)\n",
    "        # read external from outside\n",
    "        self.external_memory = ememory\n",
    "        # generic output linear layer\n",
    "        self.generic_word_output_layer = nn.Linear(self.hidden_size,output_size)\n",
    "        # emotional output linear layer \n",
    "        self.emotion_word_output_layer = nn.Linear(self.hidden_size,output_size)\n",
    "        # emotional gate/ choice layer\n",
    "        self.alpha_layer = nn.Linear(hidden_size,1)\n",
    "        # Luong eq 5 layer\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "    def forward(self, input_step,input_step_emotion, last_hidden\n",
    "                ,input_context, encoder_outputs,last_rnn_output = None):\n",
    "        '''\n",
    "        Decoder with external memory.\n",
    "        \n",
    "        '''\n",
    "        if not torch.is_floating_point(input_step_emotion):\n",
    "            input_step_emotion = self.emotion_embedding(input_step_emotion)\n",
    "        rnn_output, hidden, new_M_emo,context = self.internal_memory(input_step,last_rnn_output,input_step_emotion,\n",
    "                                                            input_context,last_hidden,encoder_outputs)\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        concat_input = torch.cat((rnn_output, context), -1)\n",
    "        concat_output = self.concat(concat_input)\n",
    "        # concat_output = rnn_output\n",
    "        # this part is not using inside ECM (?)\n",
    "        if self.external_memory is not None:\n",
    "            # Project hidden output to distribution.\n",
    "            generic_output = self.generic_word_output_layer(concat_output)\n",
    "            emotion_output = self.emotion_word_output_layer(concat_output)\n",
    "            generic_output = generic_output.squeeze(0)\n",
    "            emotion_output = emotion_output.squeeze(0)\n",
    "            # external memory gate\n",
    "            g = torch.sigmoid(self.alpha_layer(rnn_output))\n",
    "            output_g = torch.softmax(generic_output,dim = 1) * (1 - g)\n",
    "            output_e = torch.softmax(emotion_output,dim = 1) * g\n",
    "            output = output_g + output_e # output distribution\n",
    "            output = output.squeeze(0)\n",
    "            g = torch.cat([(1 - g),g],dim = -1) # gate distribution\n",
    "            g = g.squeeze(0)\n",
    "        else:\n",
    "            # Predict next word using Luong eq. 6\n",
    "            output = self.out(concat_output).squeeze(0)\n",
    "            # generic output\n",
    "            output = F.softmax(output, dim=1)\n",
    "            output = output.squeeze(0)\n",
    "            g = None\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden, new_M_emo, context,concat_output,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLL Loss + Internal Memory Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.417398Z",
     "start_time": "2019-04-03T00:20:22.413116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maskNLLLoss_IMemory(inp, target, mask,M_emo,external_memory,alpha):\n",
    "    '''\n",
    "    When external memory input will be a tuple with 4 elements\n",
    "    '''\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    # cross entropy loss\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    # internal emotional loss\n",
    "    eos_mask = (target == 2) # 2 is EOS token\n",
    "    eos_mask = eos_mask.type_as(M_emo)\n",
    "    internal_memory_loss = torch.norm(M_emo,dim = 2) * eos_mask\n",
    "    internal_memory_loss = internal_memory_loss.squeeze(0)\n",
    "    # external\n",
    "    # find 1,0\n",
    "    if external_memory is not None:\n",
    "        qt = torch.gather(external_memory.view(-1,1),0,target.view(-1,1)).type(torch.LongTensor)\n",
    "        qt = qt.to(device)\n",
    "        alpha_prob = torch.gather(alpha,1,qt) # if it select emotion word or generic word\n",
    "        external_memory_loss = (-torch.log(alpha_prob)).reshape(-1) \n",
    "    else:\n",
    "        external_memory_loss = torch.zeros(crossEntropy.shape,dtype=torch.float,device=device)\n",
    "    #print(crossEntropy.masked_select(mask).mean(),internal_memory_loss.masked_select(mask).mean())\n",
    "    loss = crossEntropy.masked_select(mask).mean() + external_memory_loss.mean() + internal_memory_loss.mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item(),crossEntropy.masked_select(mask).mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.432980Z",
     "start_time": "2019-04-03T00:20:22.419994Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "def train(input_variable, lengths, target_variable,target_variable_emotion,\n",
    "          mask, max_target_len, encoder, decoder, embedding,emotion_embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # num_samples in this batch\n",
    "    num_samples = input_variable.shape[1]\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    target_variable_emotion = target_variable_emotion.to(device)\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    totalCrossEntropy = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(num_samples)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    # Set initial context value,last_rnn_output, internal_memory\n",
    "    context_input = torch.zeros(num_samples,hidden_size,dtype=torch.float,device=device) #torch.FloatTensor(batch_size,hidden_size)\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    if random.random() < teacher_forcing_ratio:\n",
    "        use_teacher_forcing = True  \n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "    use_teacher_forcing = False\n",
    "    # initialize value for rnn_output\n",
    "    rnn_output = None\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden,target_variable_emotion,context_input,rnn_output,g = decoder(\n",
    "                decoder_input,target_variable_emotion, decoder_hidden,\n",
    "                context_input, encoder_outputs,rnn_output\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal,crossEntropy = maskNLLLoss_IMemory(decoder_output, target_variable[t], mask[t],target_variable_emotion,decoder.external_memory,g)\n",
    "            loss += mask_loss\n",
    "            totalCrossEntropy += crossEntropy * nTotal\n",
    "            print_losses.append(mask_loss.item() * nTotal) # print average loss\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden,target_variable_emotion,context_input,rnn_output,g = decoder(\n",
    "                decoder_input,target_variable_emotion, decoder_hidden,\n",
    "                context_input,encoder_outputs,rnn_output\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            topi = topi.squeeze(0)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(num_samples)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal,crossEntropy = maskNLLLoss_IMemory(decoder_output, target_variable[t], mask[t],target_variable_emotion,decoder.external_memory,g)\n",
    "            loss += mask_loss\n",
    "            totalCrossEntropy += crossEntropy * nTotal\n",
    "            print_losses.append(mask_loss.item() * nTotal) # print average loss\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    #print('Total Loss {}; Cross Entropy: {}'.format(sum(print_losses) / n_totals, totalCrossEntropy / n_totals))\n",
    "    return sum(print_losses) / n_totals,totalCrossEntropy / n_totals\n",
    "def evaluate_performance(input_variable, lengths, target_variable,target_variable_emotion,\n",
    "          mask, max_target_len, encoder, decoder):\n",
    "    # test mode\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # num_samples in this batch\n",
    "    num_samples = input_variable.shape[1]\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    target_variable_emotion = target_variable_emotion.to(device)\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    totalCrossEntropy = 0\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(num_samples)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    # Set initial context value,last_rnn_output, internal_memory\n",
    "    context_input = torch.zeros(num_samples,hidden_size,dtype=torch.float,device=device) #torch.FloatTensor(batch_size,hidden_size)\n",
    "    # initial value for rnn output\n",
    "    rnn_output = None\n",
    "    # forward pass to generate all sentences\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden,target_variable_emotion,context_input,rnn_output,g = decoder(\n",
    "            decoder_input,target_variable_emotion, decoder_hidden,\n",
    "            context_input,encoder_outputs,rnn_output\n",
    "        )\n",
    "        # No teacher forcing: next input is decoder's own current output\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        topi = topi.squeeze(0)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(num_samples)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal,crossEntropy = maskNLLLoss_IMemory(decoder_output, target_variable[t], mask[t],target_variable_emotion,decoder.external_memory,g)\n",
    "        loss += mask_loss\n",
    "        totalCrossEntropy += (crossEntropy * nTotal)\n",
    "        print_losses.append(mask_loss.item() * nTotal) # print average loss\n",
    "        n_totals += nTotal\n",
    "    # back to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    return sum(print_losses) / n_totals, totalCrossEntropy / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.448790Z",
     "start_time": "2019-04-03T00:20:22.435338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs,pairs_emotion, \n",
    "               encoder, decoder, encoder_optimizer,\n",
    "               decoder_optimizer, embedding,emotion_embedding, \n",
    "               encoder_n_layers, decoder_n_layers, save_dir, \n",
    "               n_iteration, batch_size, print_every, save_every, \n",
    "               clip,corpus_name,external_memory,test_pairs,test_pairs_emotion):\n",
    "    loadFilename=None\n",
    "    # Load batches for each iteration\n",
    "    #training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      #for _ in range(n_iteration)]\n",
    "    print('Loading Training data ...')\n",
    "    length_pairs = len(pairs)\n",
    "    #training_batches = [batch2TrainData(voc, [random.choice(range(length_pairs)) for _ in range(batch_size)],\n",
    "    #                                   pairs,pairs_emotion) for _ in range(n_iteration)]\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    totalCrossEntropy = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "    min_test_loss = 1000\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = batch2TrainData(voc, [random.choice(range(length_pairs)) for _ in range(batch_size)],\n",
    "                                       pairs,pairs_emotion)\n",
    "        # to save the data that causes error\n",
    "        #with open('wrong_data.pickle','rb') as f:\n",
    "        #    training_batch = pickle.load(f)\n",
    "        \n",
    "        # Extract fields from batch\n",
    "        input_variable,input_variable_emotion, lengths, target_variable,target_variable_emotion, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss,crossEntropy = train(input_variable, lengths, target_variable,target_variable_emotion,\n",
    "                     mask, max_target_len, encoder,\n",
    "                     decoder, embedding,emotion_embedding,\n",
    "                     encoder_optimizer, decoder_optimizer, \n",
    "                     batch_size, clip)\n",
    "        \n",
    "        print_loss += loss\n",
    "        totalCrossEntropy += crossEntropy\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0 or iteration == 1:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            if iteration == 1:\n",
    "                print_loss_avg = print_loss / 1\n",
    "                print_cross_entropy = totalCrossEntropy / 1\n",
    "            else:\n",
    "                print_loss_avg = print_loss / print_every\n",
    "                print_cross_entropy = totalCrossEntropy / print_every\n",
    "            if print_cross_entropy > 300:\n",
    "                perplexity = compute_perplexity(300)\n",
    "            else:\n",
    "                perplexity = compute_perplexity(print_cross_entropy)\n",
    "            output1 = \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Perplexity: {:.2f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg,perplexity)\n",
    "            print(output1)\n",
    "            test_length_pairs = len(test_pairs) \n",
    "            test_batch = batch2TrainData(voc, [random.choice(range(test_length_pairs)) for _ in range(batch_size)],\n",
    "                                       test_pairs,test_pairs_emotion)\n",
    "            input_variable,input_emotion, lengths, target_variable,target_emotion, mask, max_target_len = test_batch\n",
    "            test_loss,testCrossEntropy = evaluate_performance(input_variable,lengths, target_variable,target_emotion,mask,max_target_len,encoder,decoder)\n",
    "            \n",
    "            if testCrossEntropy > 300:\n",
    "                perplexity = compute_perplexity(300)\n",
    "            else:\n",
    "                perplexity = compute_perplexity(testCrossEntropy)\n",
    "            output2 = 'Loss on validation set {:.4f}; Perplexity:{:.2f}'.format(test_loss,perplexity)\n",
    "            print(output2)\n",
    "            with open(os.path.join(directory,'log.txt'),'a+') as f:\n",
    "                f.write(output1 + '\\n')\n",
    "                f.write(output2 + '\\n')\n",
    "            print_loss = 0\n",
    "            totalCrossEntropy = 0\n",
    "\n",
    "        # Save checkpoint and only save the better perform one,\n",
    "        if (iteration % save_every == 0) and (testCrossEntropy < min_test_loss):\n",
    "            min_test_loss = testCrossEntropy\n",
    "            print('Save the model at checkpoint {}, and test loss is {}'.format(iteration,min_test_loss))\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict(),\n",
    "                'external_memory':external_memory\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_param(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        print(param)\n",
    "        print(name,param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.459565Z",
     "start_time": "2019-04-03T00:20:22.451396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder,num_word = None):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq,target_emotions,input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Set initial context value,last_rnn_output, internal_memory\n",
    "        context_input = torch.zeros((1,hidden_size),dtype=torch.float,device=self.decoder.device)\n",
    "        context_input = context_input.to(device)\n",
    "        rnn_output = None\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden,target_emotions,context_input,rnn_output,g = decoder(\n",
    "                decoder_input,target_emotions, decoder_hidden,\n",
    "                context_input, encoder_outputs,rnn_output\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.474049Z",
     "start_time": "2019-04-03T00:20:22.461944Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BeamSearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder,num_word):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.num_word = num_word\n",
    "\n",
    "    def forward(self, input_seq,target_emotions,input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_words_order = torch.zeros((1,self.num_word),device=decoder.device,dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        all_scores_array = torch.zeros((1,self.num_word),device=decoder.device,dtype=torch.float)\n",
    "        # Set initial context value,last_rnn_output, internal_memory\n",
    "        context_input = torch.zeros(1,hidden_size,dtype=torch.float)\n",
    "        context_input = context_input.to(decoder.device)\n",
    "        rnn_output = None\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden,target_emotions,context_input,rnn_output,g = decoder(\n",
    "                decoder_input,target_emotions, decoder_hidden,\n",
    "                context_input, encoder_outputs,rnn_output\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            decoder_input_order = torch.argsort(decoder_output,dim=1,descending=True)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            all_scores_array = torch.cat((all_scores_array,decoder_output),dim = 0)\n",
    "            all_words_order = torch.cat((all_words_order,decoder_input_order), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        sequences = self.beam_search(all_scores_array,3)\n",
    "        return sequences\n",
    "    def beam_search(self,array,k):\n",
    "        array = array.tolist()\n",
    "        sequences = [[list(), 1.0]]\n",
    "        # walk over each step in sequence\n",
    "        for row in array:\n",
    "            all_candidates = list()\n",
    "            # expand each current candidate\n",
    "            for i in range(len(sequences)):\n",
    "                seq, score = sequences[i]\n",
    "                for j in range(len(row)):\n",
    "                    candidate = [seq + [j], score - np.log(row[j] + 1e-8)]\n",
    "                    all_candidates.append(candidate)\n",
    "            # order all candidates by score\n",
    "            ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "            # select k best\n",
    "            sequences = ordered[:k]\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:22.688663Z",
     "start_time": "2019-04-03T00:20:22.476414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion word counts: 611\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    voc\n",
    "except NameError:\n",
    "    voc = Voc('a',max_length=MAX_LENGTH,min_count=MIN_COUNT)\n",
    "# Configure models\n",
    "model_name = 'emotion_model'\n",
    "corpus_name = 'ECM10_words_Ememory_concat'\n",
    "attn_model = 'concat'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "# number of emotion\n",
    "num_emotions = 5\n",
    "# load external memory based vocab.\n",
    "emotion_words = get_ememory('ememory2.txt',voc)\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = 'data/save/emotion_model/ECM10_words_Ememory_concat/2-2_500/4100_checkpoint.tar'\n",
    "checkpoint_iter = 120\n",
    "training = True\n",
    "if loadFilename:\n",
    "    training = False\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "    emotion_words = checkpoint['external_memory']\n",
    "    \n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "if emotion_words is not None:\n",
    "    emotion_words = emotion_words.to(device)\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "emotion_embedding = nn.Embedding(num_emotions, hidden_size)\n",
    "emotion_embedding_static = nn.Embedding(num_emotions,hidden_size)\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding,emotion_embedding, hidden_size, \n",
    "                              voc.num_words,device, emotion_words,decoder_n_layers, dropout,num_emotions=num_emotions,batch_size = batch_size)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "    \n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(536, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_words.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:45.601591Z",
     "start_time": "2019-04-03T00:20:22.690677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Loading Training data ...\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 0.0%; Average loss: 1.7481; Perplexity: 4.81\n",
      "Loss on validation set 1.8860; Perplexity:5.58\n",
      "Iteration: 20; Percent complete: 0.0%; Average loss: 1.7888; Perplexity: 5.12\n",
      "Loss on validation set 1.6075; Perplexity:4.20\n",
      "Iteration: 40; Percent complete: 0.0%; Average loss: 1.9354; Perplexity: 5.90\n",
      "Loss on validation set 1.9819; Perplexity:6.28\n",
      "Iteration: 60; Percent complete: 0.0%; Average loss: 1.9074; Perplexity: 5.70\n",
      "Loss on validation set 2.0337; Perplexity:6.22\n",
      "Iteration: 80; Percent complete: 0.0%; Average loss: 1.8984; Perplexity: 5.67\n",
      "Loss on validation set 1.9493; Perplexity:5.85\n",
      "Iteration: 100; Percent complete: 0.0%; Average loss: 1.8182; Perplexity: 5.25\n",
      "Loss on validation set 1.9480; Perplexity:5.82\n",
      "Save the model at checkpoint 100, and test loss is 1.761008028690186\n",
      "Iteration: 120; Percent complete: 0.0%; Average loss: 1.7862; Perplexity: 5.07\n",
      "Loss on validation set 1.7056; Perplexity:4.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '<ipython-input-12-ed88bde37548>'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7ed203a5d87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m trainIters(model_name, voc, pairs,pairs_emotion, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[1;32m     28\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m            print_every, save_every, clip,corpus_name,emotion_words,test_pairs,test_pairs_emotion)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a1c186fa223d>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, pairs_emotion, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, emotion_embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, external_memory, test_pairs, test_pairs_emotion)\u001b[0m\n\u001b[1;32m     38\u001b[0m                      \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                      \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                      batch_size, clip)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ed88bde37548>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, target_variable_emotion, mask, max_target_len, encoder, decoder, embedding, emotion_embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     60\u001b[0m             decoder_output, decoder_hidden,target_variable_emotion,context_input,rnn_output,g = decoder(\n\u001b[1;32m     61\u001b[0m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_variable_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mcontext_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             )\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# No teacher forcing: next input is decoder's own current output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5dd5723723ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, input_step_emotion, last_hidden, input_context, encoder_outputs, last_rnn_output)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moutput_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_e\u001b[0m \u001b[0;31m# output distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gate distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \"\"\"\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# First call the orignal checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50\n",
    "teacher_forcing_ratio = 0.1\n",
    "learning_rate = 0.001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 2000000\n",
    "print_every = 20\n",
    "save_every = 100\n",
    "\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs,pairs_emotion, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding,emotion_embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip,corpus_name,emotion_words,test_pairs,test_pairs_emotion)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:45.602651Z",
     "start_time": "2019-04-03T00:20:12.272Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, emotions,max_length=MAX_LENGTH,beam_search = False):\n",
    "    emotions = int(emotions)\n",
    "    emotions = torch.LongTensor([emotions])\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    emotions = emotions.to(device)\n",
    "\n",
    "    # indexes -> words\n",
    "    if beam_search:\n",
    "        sequences = searcher(input_batch, emotions, lengths, max_length)\n",
    "        decoded_words = beam_decode(sequences,voc)\n",
    "    else:\n",
    "        # Decode sentence with searcher\n",
    "        tokens, scores = searcher(input_batch, emotions, lengths, max_length)\n",
    "        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def beam_decode(sequences,voc):\n",
    "    for each in sequences:\n",
    "        for idxs in each:\n",
    "            return [voc.index2word[idx] for idx in idxs[:-1]]\n",
    "    \n",
    "def evaluateInput(encoder, decoder, searcher, voc,emotion_dict,beam_search):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            for emotion in range(len(emotion_dict)):\n",
    "                # Check if it is quit case\n",
    "                if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "                # Normalize sentence\n",
    "                input_sentence = normalizeString(input_sentence)\n",
    "                # Evaluate sentence\n",
    "                output_words = evaluate(encoder, decoder, searcher, voc, input_sentence,emotion,beam_search=beam_search)\n",
    "                # Format and print response sentence\n",
    "                output=[]\n",
    "                for word in output_words:\n",
    "                    if word == 'PAD':\n",
    "                        continue\n",
    "                    elif word == 'EOS':\n",
    "                        break\n",
    "                    else:\n",
    "                        output.append(word)\n",
    "                print('Bot({}):'.format(emotion_dict[emotion]), ' '.join(output))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentenceFromIdx(idx,voc):\n",
    "    return ' '.join([voc.index2word[i] for i in idx])\n",
    "\n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, decoder_input, logProb, length,emotions_emb,last_rnn_output,context_input,g):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        \n",
    "        self.hidden_state = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.decoder_input = decoder_input\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "        self.emotions = emotions_emb\n",
    "        self.rnn_output = last_rnn_output\n",
    "        self.context_input = context_input\n",
    "        self.alpha = g\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post(neutral):is this camera a gift for someone ?\n",
      "it the for my EOS camera . saturday ? . 2.438005160759483\n",
      "no the for my EOS camera . saturday ? . 2.537748231145636\n",
      "please the for my EOS camera . saturday ? . 2.579818670605086\n",
      "this the for my EOS camera . saturday ? . 2.6134734462043707\n",
      "the the for my EOS camera . saturday ? . 2.6652186191011746\n",
      "i the for my EOS camera . saturday ? . 2.680353522095937\n",
      "and the for my EOS camera . saturday ? . 2.7408142335768813\n",
      "let the for my EOS camera . saturday ? . 2.8000386976096676\n",
      "for the for my EOS camera . saturday ? . 2.824786986801574\n",
      "it the for my EOS camera . saturday ? ? 3.1086104237326926\n",
      "Post(joy):is this camera a gift for someone ?\n",
      "it the for my EOS camera . saturday ? . 2.4371323023646414\n",
      "no the for my EOS camera . saturday ? . 2.5367508251620623\n",
      "please the for my EOS camera . saturday ? . 2.5785227393634993\n",
      "this the for my EOS camera . saturday ? . 2.611842759231034\n",
      "the the for my EOS camera . saturday ? . 2.66439468595784\n",
      "i the for my EOS camera . saturday ? . 2.679506806365399\n",
      "and the for my EOS camera . saturday ? . 2.739762938013703\n",
      "let the for my EOS camera . saturday ? . 2.7995740946446532\n",
      "for the for my EOS camera . saturday ? . 2.8233930251390364\n",
      "it the for my EOS camera . saturday ? ? 3.109566209647396\n",
      "Post(anger):is this camera a gift for someone ?\n",
      "it the for my EOS camera . saturday ? . 2.394084436527024\n",
      "no the for my EOS camera . saturday ? . 2.539437141791783\n",
      "this the for my EOS camera . saturday ? . 2.5422053322461458\n",
      "please the for my EOS camera . saturday ? . 2.6225357529652533\n",
      "the the for my EOS camera . saturday ? . 2.6748749806803924\n",
      "i the for my EOS camera . saturday ? . 2.68263710682797\n",
      "and the for my EOS camera . saturday ? . 2.7254008024902427\n",
      "we the for my EOS camera . saturday ? . 2.7724478790273217\n",
      "for the for my EOS camera . saturday ? . 2.784902395030618\n",
      "it the for my EOS camera . saturday ? ? 3.064729285650172\n",
      "Post(sadness):is this camera a gift for someone ?\n",
      "it the for my EOS camera . saturday ? . 2.42308504156922\n",
      "no the for my EOS camera . saturday ? . 2.539920638778717\n",
      "this the for my EOS camera . saturday ? . 2.5699440911368607\n",
      "please the for my EOS camera . saturday ? . 2.59626655967018\n",
      "the the for my EOS camera . saturday ? . 2.673341693567877\n",
      "i the for my EOS camera . saturday ? . 2.6862014435319903\n",
      "and the for my EOS camera . saturday ? . 2.7167869949649432\n",
      "let the for my EOS camera . saturday ? . 2.7835184970436995\n",
      "for the for my EOS camera . saturday ? . 2.7964692243330123\n",
      "it the for my EOS camera . saturday ? ? 3.093032702818195\n",
      "Post(fear):is this camera a gift for someone ?\n",
      "it the for my EOS camera . saturday ? . 2.436757104551983\n",
      "no the for my EOS camera . saturday ? . 2.535465793265163\n",
      "please the for my EOS camera . saturday ? . 2.578412389854552\n",
      "this the for my EOS camera . saturday ? . 2.6120655001330726\n",
      "the the for my EOS camera . saturday ? . 2.639988831073588\n",
      "i the for my EOS camera . saturday ? . 2.6795605115476855\n",
      "and the for my EOS camera . saturday ? . 2.7390503412668266\n",
      "let the for my EOS camera . saturday ? . 2.7988794375809576\n",
      "for the for my EOS camera . saturday ? . 2.8235144725716554\n",
      "it the for my EOS camera . saturday ? ? 3.109376406103308\n"
     ]
    }
   ],
   "source": [
    "for emotions in [0,1,2,3,4]:\n",
    "    diversity_penality = True\n",
    "    emotions = emotions\n",
    "    sentence = 'is this camera a gift for someone ?'\n",
    "    print('Post({}):{}'.format(emo_dict[emotions],sentence))\n",
    "    emotions = int(emotions)\n",
    "    emotions = torch.LongTensor([emotions])\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    emotions = emotions.to(device)\n",
    "    # Forward input through encoder model\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batch, lengths)\n",
    "    # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    # Initialize decoder input with SOS_token\n",
    "    decoder_input = torch.ones((1,1), device=device, dtype=torch.long) * SOS_token\n",
    "    # Set initial context value,last_rnn_output, internal_memory\n",
    "    context_input = torch.zeros(1,hidden_size,dtype=torch.float)\n",
    "    context_input = context_input.to(decoder.device)\n",
    "    rnn_output = None\n",
    "\n",
    "    node = BeamSearchNode(hiddenstate=decoder_hidden,decoder_input=decoder_input,\n",
    "                           context_input=context_input,emotions_emb=emotions,\n",
    "                           length=1,logProb=0,last_rnn_output = rnn_output,\n",
    "                           previousNode=None,g = 0\n",
    "                          )\n",
    "    sent_leng = 0\n",
    "    # beam search\n",
    "    K = 10\n",
    "    # Iteratively decode one word token at a time\n",
    "    # Forward pass through decoder\n",
    "    nodes = PriorityQueue(maxsize=K)\n",
    "    nodes.put((0,node))\n",
    "    # diversity rate\n",
    "    gamma = 10\n",
    "    # choice\n",
    "    g_losses = []\n",
    "    for i in range(10):\n",
    "        #print('Decoder {} word'.format(i + 1))\n",
    "        choices = []\n",
    "        while not nodes.empty():\n",
    "            score,node = nodes.get()\n",
    "            #print('Last word at position {}'.format(node.leng))\n",
    "            decoder_output, decoder_hidden,emotions,context_input,rnn_output,g = decoder(\n",
    "                node.decoder_input,node.emotions, node.hidden_state,\n",
    "                node.context_input,encoder_outputs,node.rnn_output\n",
    "            )\n",
    "            #print(g)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            # decoder_output = decoder_output.unsqueeze(0)\n",
    "            decoder_scores, decoder_input = torch.topk(decoder_output,k= K, dim=1)\n",
    "            decoder_scores = torch.log(decoder_scores)\n",
    "            if diversity_penality and i >= 1:\n",
    "                # apply based on rank\n",
    "                penalties = torch.pow(torch.arange(0,K,dtype=torch.float,device=device),3) * gamma\n",
    "                # apply penalties on the output\n",
    "                decoder_scores = decoder_scores - penalties\n",
    "            token_choices = [decoder_input[0,i].item() for i in range(1,K)] \n",
    "            token_scores = [decoder_scores[0,i].item() for i in range(1,K)] \n",
    "            #print(voc.index2word[token_choices[0]])\n",
    "            # for each candidate token, compute loss\n",
    "            for token,decoder_score in zip(token_choices,token_scores):\n",
    "                next_decoder_input = torch.ones((1,1),dtype=torch.long,device=device) * token\n",
    "                #current_score = score + decoder_score\n",
    "                next_node = BeamSearchNode(decoder_hidden,node,next_decoder_input,\n",
    "                                      decoder_score,node.leng + 1,emotions,rnn_output,context_input,g)\n",
    "                #print('This is {} words'.format(next_node.leng))\n",
    "                current_score = (score * node.leng - next_node.eval()) / next_node.leng\n",
    "                choices.append((current_score,next_node))\n",
    "        choices = sorted(choices,key=lambda x:x[0])\n",
    "        # choices = choices[:K]\n",
    "        for choice in choices:\n",
    "            if not nodes.full():\n",
    "                nodes.put(choice)\n",
    "\n",
    "    #print(nodes.qsize())\n",
    "    #print('Decode')        \n",
    "    # decoder    \n",
    "    sentences = []\n",
    "    i = 0\n",
    "    while not nodes.empty():\n",
    "        #print('Decode {}:'.format(i))\n",
    "        i += 1 \n",
    "        sentence = []\n",
    "        score,node = nodes.get()\n",
    "        while(node.prevNode is not None):\n",
    "            sentence.append(node.decoder_input.item())\n",
    "            node = node.prevNode\n",
    "        sentence = sentence[::-1]\n",
    "        #print(sentence,score)\n",
    "        sentences.append((score,sentence))\n",
    "    #print(sentences)\n",
    "    for sent in sentences[:40]:\n",
    "        print(sentenceFromIdx(sent[1],voc),sent[0])  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.word2index['be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['did you call me ?', 'thank you for returning my call .'],\n",
       " ['in cash .', 'is that us dollars ?'],\n",
       " ['no don t use them please .', 'oh all right sir .'],\n",
       " ['room service . may i come in ?', 'come in please .'],\n",
       " ['i want to open another account .', 'what kind would you like to open ?'],\n",
       " ['yes sir ?', 'could we have the bill please ?'],\n",
       " ['mam another minute could i ?', 'hurry up little boy .'],\n",
       " ['we d like to have typical chinese food .',\n",
       "  'anything you d rather not have ?'],\n",
       " ['no .', 'no .'],\n",
       " ['is this camera a gift for someone ?',\n",
       "  'no it s my camera for personal use .'],\n",
       " ['hi mary .', 'you seem to be in a hurry .'],\n",
       " ['it s not serious .', 'are you sure ?'],\n",
       " ['i m still feeling under the weather .',\n",
       "  'would you like anything for your stomach ?'],\n",
       " ['bye !', 'bye chandler ! i miss you already .'],\n",
       " ['at counter . do you take parcels here ?', 'yes .'],\n",
       " ['western food or chinese food ?', 'chinese food .'],\n",
       " ['this is your table .', 'and would you like to order now ?'],\n",
       " ['hi kevin how was your year at college ?',\n",
       "  'it was great ! how was your year ?'],\n",
       " ['hold on please .', 'thanks .'],\n",
       " ['margaret mitchell .', 'oh yes .here it is .'],\n",
       " ['when will you be able to fix it ?',\n",
       "  'how does this afternoon at sound to you ?'],\n",
       " ['which bus should i take to railway station ?',\n",
       "  'you can take a no . bus here .'],\n",
       " ['i thought you already have a job .',\n",
       "  'and people say you don t pay attention .'],\n",
       " ['yes i like using chopsticks .', 'do you like some soup ?'],\n",
       " ['oh dear my weight has gone up again .',\n",
       "  'i am not surprised you eat too much .'],\n",
       " ['but then this guy s right after him .', 'hello !'],\n",
       " ['what do you do in your spare time ?', 'i listen to music and read .'],\n",
       " ['hey !', 'hey !'],\n",
       " ['so is this your first time to taiwan ?', 'no i first came here .'],\n",
       " ['i m getting sleepy .', 'you ve had so long a flight .'],\n",
       " ['everything was delicious !', 'thank you !'],\n",
       " ['i will be voting tomorrow .', 'what s your polling place ?'],\n",
       " ['oh so do i .', 'why don t we go for one now ?'],\n",
       " ['okay .', 'all right .'],\n",
       " ['take this medicine three times a day .', 'yes .'],\n",
       " ['one hundred dollars .', 'here is the money .'],\n",
       " ['i need to see your id .', 'i think i forgot it in the car .'],\n",
       " ['ross has a boyfriend .', 'i do i do'],\n",
       " ['i ve got some cigars .', 'how many madam ?'],\n",
       " ['okay open ours next . open ours next !', 'okay .'],\n",
       " ['no thanks .', 'bye .'],\n",
       " ['look after yourself . bye .', 'bye .'],\n",
       " ['yes . here is my passport .', 'what currency will you have ?'],\n",
       " ['yeah i ve been out of town .', 'so how have you been ?'],\n",
       " ['how much is the rent ?', 'the rent is per month .'],\n",
       " ['what about your brother ?', 'he s not quite .'],\n",
       " ['thanks a lot .', 'do you work in shanghai ?'],\n",
       " ['do you think this music is warm matched ?', 'of course yes .'],\n",
       " ['who s that man standing next to her ?', 'which man ?'],\n",
       " ['sightseeing .', 'how long are you staying here ?'],\n",
       " ['ahh .', 'oh would you look at that monica ?'],\n",
       " ['how about ?', 'that ll be fine . see you soon .'],\n",
       " ['i already did .', 'is it on the shelf ?'],\n",
       " ['we d like a non smoking room please .', 'no problem . follow me please .'],\n",
       " ['do you have any more questions ?', 'no sir .'],\n",
       " ['last night .', 'where did the break in happen ?'],\n",
       " ['it s my acceptance letter from ucla .', 'what does it say ?'],\n",
       " ['i think i m crazy about her .', 'you are why not ask for a date ?'],\n",
       " ['take it three times a day .', 'can i take it with food ?'],\n",
       " ['i didn t get in .', 'you re lying .'],\n",
       " ['do you mind telling me where it is ?',\n",
       "  'of course which room number is it ?'],\n",
       " ['that would be too late .', 'maybe the situation is too urgent .'],\n",
       " ['are you really interested in doing something ?',\n",
       "  'i don t mind if you want to .'],\n",
       " ['oh o . k . where s that ?', 'it s at the corner of that street .'],\n",
       " ['hello . . can i help you ?', 'i need the police .'],\n",
       " ['do you know what you want to get ?', 'a soda sounds good .'],\n",
       " ['doesn t look like it to me .', 'just get out of my face !'],\n",
       " ['how much time do you need ?', 'give me one more day .'],\n",
       " ['i am afraid there is none .', 'then i ll have vanilla ice cream .'],\n",
       " ['that s a pity . how about thursday ?', 'that would be fine .'],\n",
       " ['until then .', 'talk to you later .'],\n",
       " ['what can i see in beijing ?', 'you can see the forbidden city .'],\n",
       " ['i want to take out .',\n",
       "  'which account are you making this withdrawal from ?'],\n",
       " ['how many shifts are there in your factory ?',\n",
       "  'there are two shifts now in total .'],\n",
       " ['i think i ll be a little afraid .', 'of what ?'],\n",
       " ['six hundred us dollars .', 'i m sorry sir after'],\n",
       " ['that s more like it .', 'tell me about it .'],\n",
       " ['ok i ll expected you around o clock ?', 'see you then .'],\n",
       " ['i would like to get that .',\n",
       "  'would you like anything else with your package ?'],\n",
       " ['you want to send it airmail madam ?', 'i do want to send it airmail yes .'],\n",
       " ['thanks for your help .', 'no problem anytime .'],\n",
       " ['fine i ll be expecting you .', 'see you tomorrow .'],\n",
       " ['fine thank you and you ?', 'pretty good . what brings you here ?'],\n",
       " ['well . have a pleasant stay in germany .', 'thank you .'],\n",
       " ['i heard many stories about your brother .', 'he s a hero in our town .'],\n",
       " ['why are you so excited ?', 'i just got done voting .'],\n",
       " ['uh i m gonna kill myself !', 'i ll get back to ya .'],\n",
       " ['did you call the repairman ?', 'of course .'],\n",
       " ['could you leave him a message ?', 'sure .'],\n",
       " ['could you use wooden cases instead ?', 'why use wooden cases ?'],\n",
       " ['alright ?', 'we we had our first fight this morning .'],\n",
       " ['can i help you sir ?', 'i want to have a haircut .'],\n",
       " ['alright i ll bring it in this afternoon .',\n",
       "  'no problem . see you this afternoon .'],\n",
       " ['and to drink ?', 'i ll have a diet coke please .'],\n",
       " ['is that everything that i have to do ?', 'that s all there is .'],\n",
       " ['ok . what time ?', 'please .'],\n",
       " ['hey !', 'hey !'],\n",
       " ['well . could i try it on ?', 'sure .'],\n",
       " ['so you have your own car ?', 'i sure do .'],\n",
       " ['i would like to deposit a check .', 'you need to sign the back .'],\n",
       " ['make a left on this next street .', 'tell me how far to go .'],\n",
       " ['yes . can i ?', 'do you really need to ?'],\n",
       " ['here let me get the door for you .', 'thanks .'],\n",
       " ['merry christmas linda !', 'merry christmas lee !'],\n",
       " ['i can do that .', 'that s great . thanks a lot .'],\n",
       " ['excuse me . is there any body here ?', 'yes .'],\n",
       " ['no i m not .', 'what s your job ?'],\n",
       " ['do you like playing table tennis ?', 'certainly .'],\n",
       " ['dreadful . there the door is open now .', 'thank you .'],\n",
       " ['yeah .', 'does it still hurt ?'],\n",
       " ['yes apparently four stores were broken into .',\n",
       "  'did the looters get caught ?'],\n",
       " ['it was only about for a inch screen .',\n",
       "  'does it pick up any digital channels ?'],\n",
       " ['let s find out what time it starts .', 'let s look at the newspaper .'],\n",
       " ['hello may i speak to mary please ?', 'speaking who s calling please ?'],\n",
       " ['i m in pretty bad shape doctor ford .', 'oh in what way .'],\n",
       " ['would you like some drinks ?', 'yes what kinds of wine do you have ?'],\n",
       " ['i m actually in school right now .', 'which school do you attend ?'],\n",
       " ['i am .', 'what has made you so happy ?'],\n",
       " ['i have a court date coming up .', 'what are you being charged with ?'],\n",
       " ['oh yes . here it is .', 'do you want large ones or small ones ?'],\n",
       " ['well that s okay .', 'yeah sorry .'],\n",
       " ['eggs milk bread . things like that .', 'go make that list .'],\n",
       " ['in the van .', 'what about other expenses ?'],\n",
       " ['good morning miss .', 'good morning i d like a haircut .'],\n",
       " ['how are you mr . zhang ?', 'very well thank you . and you ?'],\n",
       " ['what s that ?', 'could i get back the sweater you borrowed ?'],\n",
       " ['i like pop music very much .', 'what ?'],\n",
       " ['he must be very brave .', 'exactly !'],\n",
       " ['don t be silly .', 'i m serious .'],\n",
       " ['not too tight in the waist ?', 'no ! it s a perfect fit !'],\n",
       " ['sorry i can t dance .', 'never mind i can teach you .'],\n",
       " ['that s all right . enjoy your flight .', 'thank you .'],\n",
       " ['good morning .', 'good morning .'],\n",
       " ['yes my engine warning light is on .', 'how long has it been on for ?'],\n",
       " ['is this your new teacher ?', 'yes it is .'],\n",
       " ['how about next friday ?', 'friday sounds good .'],\n",
       " ['no they come every hour on the hour .', 'thank you very much .'],\n",
       " ['there s nothing left from dinner .', 'i m going to get a snack .'],\n",
       " ['excellent . have you had french food before ?', 'oh yes . marvelous !'],\n",
       " ['when will it be available ?', 'the end of this month .'],\n",
       " ['maybe we should try a thai massage too .',\n",
       "  'what s special about a thai massage ?'],\n",
       " ['do you think tom will be elected president ?',\n",
       "  'no i think harry will get it .'],\n",
       " ['he wanted to take it home yesterday .', 'i wonder what he ll name it .'],\n",
       " ['no we just don t usually admit it !',\n",
       "  'well thanks for the compliment anyway !'],\n",
       " ['no thanks .', 'well i hope you ll feel better soon .'],\n",
       " ['what are you shopping for ?', 'some new clothes . how about you ?'],\n",
       " ['yes try this one please .', 'this fits me well how much is it ?'],\n",
       " ['no problem . thank you for calling me .', 'you re welcome .'],\n",
       " ['quite a lot . all them one afternoon ?', 'yes it is not easy .'],\n",
       " ['i wonder who that man is .', 'which man ?'],\n",
       " ['certainly . where are you now ?', 'i m right outside my room .'],\n",
       " ['yes quite sure .', 'then have you looked downstairs ?'],\n",
       " ['sure . how do you like your seats ?', 'ordinary one please .'],\n",
       " ['yes i often go swimming .', 'are you interested in swimming ?'],\n",
       " ['i ll accompany my wife to the hospital .', 'what s wrong with her .'],\n",
       " ['i ll take it too .', 'thank you and take care .'],\n",
       " ['good luck !', 'thanks !'],\n",
       " ['hi is molly here ?', 'yeah come on in . molly ?'],\n",
       " ['before next friday .', 'all right .'],\n",
       " ['you re so careless billy .', 'sorry mum i ll lock it right away .'],\n",
       " ['ok .', 'do you have anything to declare ?'],\n",
       " ['who s playing ?', 'it s the powell orchestra .'],\n",
       " ['yes the address is main street .', 'can you write it down for me please ?'],\n",
       " ['yes i m looking for a house .', 'to buy or to rent ?'],\n",
       " ['i sure did !', 'what candidate did you vote for ?'],\n",
       " ['who s that over here ?', 'that s the new teacher .'],\n",
       " ['the doctor said i had to be careful .', 'he was quite right .'],\n",
       " ['it s very dangerous you know .', 'i want to try something exciting .'],\n",
       " ['you mean with people on it ?', 'yes ! isn t that exciting ?'],\n",
       " ['what can i do for you ?', 'i want to get a package of cigarette .'],\n",
       " ['i m a great driver .', 'could you teach me how to drive ?'],\n",
       " ['most of them are from america and europe .',\n",
       "  'what are they doing in beijing ?'],\n",
       " ['how about this set ?', 'looks nice . okay i ll take it .'],\n",
       " ['good . let s go now .', 'all right .'],\n",
       " ['they ve completely slipped my mind .',\n",
       "  'the couple with the dogs think back .'],\n",
       " ['i need to return these books .', 'what happened to this one ?'],\n",
       " ['i hope so .', 'would you please fill out the form ?'],\n",
       " ['so you speak german don t you ?', 'yeah but i can speak english too .'],\n",
       " ['yes just as you have ordered .', 'it s very nice of you .'],\n",
       " ['i ll turn the heat up .', 'yes please do that .'],\n",
       " ['oh was she hot .', 'whoa ho .'],\n",
       " ['yes we do debit or credit ?', 'debit card .'],\n",
       " ['be patient . good food never comes fast .', 'ok !'],\n",
       " ['i m splendid . how about yourself ?', 'could be better .'],\n",
       " ['is there a problem with it ?', 'i don t need it anymore .'],\n",
       " ['do you need it for the whole weekend ?', 'we will need it for both days .'],\n",
       " ['you must be ben s mum .', 'why else would i be here ?'],\n",
       " ['how about oil or spray ?', 'no nothing of the kind .'],\n",
       " ['hi dude you look upset what s up ?',\n",
       "  'haven t been sleeping well recently .'],\n",
       " ['what ll it be today sir ?', 'fill it up . super unleaded .'],\n",
       " ['about million dollars .', 'whoa .'],\n",
       " ['that s perfect .', 'i know it is .'],\n",
       " ['oh no ! did you call the police ?', 'no .'],\n",
       " ['at p . m .', 'i ll go in time thank you .'],\n",
       " ['you can see it tomorrow .', 'i don t want to miss it today .'],\n",
       " ['yes . there are trains at and', 'what time does the get to london ?'],\n",
       " ['i think a pipe burst in my apartment .', 'in what room is this pipe ?'],\n",
       " ['oh i d rather not .', 'why not ?'],\n",
       " ['yes . that would be better .', 'fine . good bye .'],\n",
       " ['excellent . how many would you like ?', 'two thousand would be fine .'],\n",
       " ['well when does the party start ?',\n",
       "  'it s supposed to start at about eight .'],\n",
       " ['that s fine .', 'what price range are you interested in ?'],\n",
       " ['you are welcome mr . sun .', 'i really enjoyed meeting with you .'],\n",
       " ['thanks .', 'see it s right over there .'],\n",
       " ['yes madam .', 'ask daniel to give you a hand ?'],\n",
       " ['which train do you catch in the evening ?',\n",
       "  'i usually take the five thirty home .'],\n",
       " ['sorry . when was this purse purchased ?',\n",
       "  'it was purchased on the th of november at'],\n",
       " ['i m sorry .', 'ohhh . don t go .'],\n",
       " ['oh it s nothing to speak of .', 'no seriously . i m impressed .'],\n",
       " ['how much is it for my hair ?', 'just give me .'],\n",
       " ['what kind of music do you like ?', 'i like classical music . do you ?'],\n",
       " ['yes do you have any rooms available ?', 'a single ?'],\n",
       " ['do you have a reservation sir ?', 'no i am afraid we don t .'],\n",
       " ['yes of course . here it is .', 'thank you . are all these yours ?'],\n",
       " ['hopefully he will be .', 'i can t wait to vote .'],\n",
       " ['i m sure they ll be here soon .', 'yeah honey they wouldn t miss this .'],\n",
       " ['yes i ache all over .', 'are you coughing much ?'],\n",
       " ['no nothing else . see you monday .', 'see you .'],\n",
       " ['sure . here is the mirror .', 'well i don t quite like the smell .'],\n",
       " ['what s the rent for a year ?',\n",
       "  'yuan including water but not electricity and gas .'],\n",
       " ['me too . in fact i m free until', 'so if you want to meet at'],\n",
       " ['is he getting better now ?', 'i think so .thank you .'],\n",
       " ['i m fine . thank you .', 'how may i help you ?'],\n",
       " ['let me see the earrings .', 'oh honey the earrings'],\n",
       " ['ok give me two pairs in white .', 'anything else ?'],\n",
       " ['i feel sorry for her .', 'maybe this is god s will .'],\n",
       " ['here s twenty dollars .', 'do you have small bills ?'],\n",
       " ['ok here you are .', 'thanks .'],\n",
       " ['that was my favorite class .', 'you have got to be talented .'],\n",
       " ['i don t understand .', 'it s our policy .'],\n",
       " ['hi . my name is jessica .', 'nice to meet you .'],\n",
       " ['oh it s nothing to speak of .', 'no seriously . i am impressed .'],\n",
       " ['really ? that s wonderful !', 'thank you for your warning .'],\n",
       " ['good afternoon . can i help you ?',\n",
       "  'is ms april wang available at the moment ?'],\n",
       " ['i guess they broke into about four stores .',\n",
       "  'did the police find who did it ?'],\n",
       " ['four stamps please . how much is that ?',\n",
       "  'three dollars and forty cents .'],\n",
       " ['is english acceptable for you ?', 'yes .'],\n",
       " ['yuan a month .', 'could i have a look at it please ?'],\n",
       " ['what s your rate ?', 'i work for an hour .'],\n",
       " ['i will thanks for your concern .', 'you re welcome .'],\n",
       " ['okay we re not .', 'right .'],\n",
       " ['somebody broke into my house in the morning .', 'when did you find out ?'],\n",
       " ['so how was joan ?', 'i broke up with her .'],\n",
       " ['yes .', 'can i sign the lease right now ?'],\n",
       " ['please use the phone .', 'thanks .'],\n",
       " ['get better .', 'thanks a lot .'],\n",
       " ['that would be great .', 'when do you want it delivered ?'],\n",
       " ['what do you mean by us .', 'well you always say you re busy .'],\n",
       " ['which pair of jeans do you like best ?',\n",
       "  'i really like the straight legs .'],\n",
       " ['i d like a little sugar please .', 'sorry i don t have any sugar .'],\n",
       " ['could you please tell me about the cabins ?',\n",
       "  'yes all the cabins are fit for two .'],\n",
       " ['ooh now you lost me .', 'you stole the phone !'],\n",
       " ['ok . come back into the classroom class .',\n",
       "  'does the class start again mam ?'],\n",
       " ['have you met the new girl ?', 'no . have you ?'],\n",
       " ['how about seven o clock ?', 'fine .we ll be expecting you .'],\n",
       " ['i am chinese by birth .', 'where is your domicile place ?'],\n",
       " ['nothing .', 'doesn t look like it to me .'],\n",
       " ['no problem .', 'are you going to this class this morning ?'],\n",
       " ['well he sure was lucky .', 'i ll say .'],\n",
       " ['may i have your room number please ?', '.'],\n",
       " ['i said it s pm .', 'thanks .'],\n",
       " ['do you have medical insurance ?', 'yes i do . blue cross .'],\n",
       " ['how often do the buses run ?', 'they run about every five minutes .'],\n",
       " ['i suppose i do .', 'the climate here is pleasant .'],\n",
       " ['morning lee . how are you ?', 'very well thank you .'],\n",
       " ['when is the next train to shanghai ?', 'the next train will leave at o .'],\n",
       " ['sure . how would you like to pay ?', 'cash please .'],\n",
       " ['do you think tom will be elected president ?',\n",
       "  'no i think harry will get it .'],\n",
       " ['can i borrow some money ?', 'sure how much do you need ?'],\n",
       " ['yes . . .', 'can i have my please ?'],\n",
       " ['i don t like going to the theatre .', 'well what do you like doing ?'],\n",
       " ['ahh i d throw another thousand on that .', 'why how much is that ?'],\n",
       " ['y know what ?', 'she d she d love this .'],\n",
       " ['i need to speak with david lin .', 'he s out of his office .'],\n",
       " ['thank you .', 'filing system ?'],\n",
       " ['yes . the scenery there is so breathtaking', 'really ?'],\n",
       " ['no .', 'this is my wedding .'],\n",
       " ['stirred will be fine .', 'here you are .'],\n",
       " ['will you take long ?', 'no ! five minutes i promise !'],\n",
       " ['do you have your library card ?', 'i sure do .'],\n",
       " ['but thank you very much all the same .', 'sorry i couldn t help you .'],\n",
       " ['hi mark .', 'oh hi stacy .'],\n",
       " ['oh that must be it .', 'well i hope you have fun tonight .'],\n",
       " ['i d rather you typed it .', 'do you want to tell her ?'],\n",
       " ['what can i do for you ?', 'i have a problem with this check .'],\n",
       " ['thank you .', 'you re welcome .'],\n",
       " ['he s quite a patient teacher too .', 'fine .'],\n",
       " ['i think about ten minutes .', 'ok we ll wait a while .'],\n",
       " ['shall we have dinner together ?',\n",
       "  'do you have a special restaurant in mind ?'],\n",
       " ['do you have a minute ?', 'well yeah sure what s up ?'],\n",
       " ['you can always make an easy breakfast .', 'what do you make ?'],\n",
       " ['we re asking for donations today .',\n",
       "  'what are you collecting donations for ?'],\n",
       " ['are you going to vote ?', 'i am . are you ?'],\n",
       " ['what s your favorite sport ?', 'swimming . what about you ?'],\n",
       " ['no .', 'yes ! totally !'],\n",
       " ['when did this happen ?', 'it happened this morning .'],\n",
       " ['please just call me tom .', 'okay tom .'],\n",
       " ['which we can celebrate later .', 'celebrate .'],\n",
       " ['dad ?', 'i m here !'],\n",
       " ['pretty fine thanks .', 'where are you going now ?'],\n",
       " ['economy class will be fine .', 'round trip or one way trip ?'],\n",
       " ['give her some money .', 'i really think they re out of rooms .'],\n",
       " ['what time is the class ?', 'from to o clock .'],\n",
       " ['really ? maybe i should work for you .',\n",
       "  'welcome . i could use the help .'],\n",
       " ['i would get del if i were you .', 'del is better than dial up ?'],\n",
       " ['well when will it be convenient for you ?',\n",
       "  'this weekend would be ok with me .'],\n",
       " ['hi francis .', 'oh mike . how are you doing ?'],\n",
       " ['i d like the lasagna please .', 'sure and which one would you like ?'],\n",
       " ['could i have a refund on this ?', 'i m afraid you can t .'],\n",
       " ['good afternoon . may i help you ?', 'yes i d like to make a deposit .'],\n",
       " ['where to ma am ?', 'the grand hotel .'],\n",
       " ['what did you order ?', 'roast beef .'],\n",
       " ['who s calling please ?', 'this is john .'],\n",
       " ['where did you buy it ?', 'i found mine at ikea .'],\n",
       " ['i feel pity for the old man .', 'i feel the same way .'],\n",
       " ['everything is ok . the commission is yuan .',\n",
       "  'here you are . thanks a lot .'],\n",
       " ['is this your first year in college ?',\n",
       "  'i actually transferred here from another school .'],\n",
       " ['what ?', 'i am'],\n",
       " ['yeah !', 'i don t know .'],\n",
       " ['honey it s just windy and raining outside .',\n",
       "  'ar ! dad the power went out .'],\n",
       " ['that ll be eight dollars please .', 'here s ten . keep the change .'],\n",
       " ['you have your library card right ?', 'yes i do .'],\n",
       " ['do you have your driver s license ?', 'sure . here it is .'],\n",
       " ['when does the library close ?', 'the library closes at six o clock .'],\n",
       " ['maybe . how much is a room ?', 'the price per night is .'],\n",
       " ['were the looters found ?', 'the police don t know who did it .'],\n",
       " ['i want to go try on these clothes .', 'what did you find ?'],\n",
       " ['. .', 'thank you very much .'],\n",
       " ['yes i do .', 'could you lend me one ?'],\n",
       " ['these are due back in two weeks .', 'all right . have a good night .'],\n",
       " ['that s really all i have to do ?', 'that s everything .'],\n",
       " ['that s too expensive for me .',\n",
       "  'this television is of very high quality .'],\n",
       " ['yes . may i help you ?', 'may i introduce myself ?'],\n",
       " ['how are you feeling ?', 'you are'],\n",
       " ['don t say it .', '. . .in love with you .'],\n",
       " ['i need a table for two for tonight .', 'what time ?'],\n",
       " ['how much does the green one cost ?', '.'],\n",
       " ['what date is it today ?', 'today is december .'],\n",
       " ['would you please bring us the check ?',\n",
       "  'sure here you are . it comes to .'],\n",
       " ['i need for my christmas cards .', 'are you sending them abroad ?'],\n",
       " ['let s do that .', 'do you know what you want to get ?'],\n",
       " ['well come over and talk to me then .', 'certainly not .'],\n",
       " ['we would prefer', 'for how many people'],\n",
       " ['thank you .', 'thank you . goodbye .'],\n",
       " ['what ?', 'i don t have health insurance .'],\n",
       " ['it is o clock now .', 'i see . what is today s schedule ?'],\n",
       " ['yeah me too .', 'so see ya on saturday .'],\n",
       " ['i think would be enough .', 'would you fill out this form please ?'],\n",
       " ['your knowledge of english is really surprising .',\n",
       "  'oh it s nothing to speak of .'],\n",
       " ['how old are you ?', 'twenty one .'],\n",
       " ['how quickly will you need your supplies ?',\n",
       "  'i need all of my supplies right away .'],\n",
       " ['i m afraid so .', 'did i do something wrong ?'],\n",
       " ['the one in the red shirt ?', 'yeah . isn t she hot ?'],\n",
       " ['i ll do that for you .', 'that s lovely .'],\n",
       " ['when are you getting married ?', 'in the spring .'],\n",
       " ['um hmm .', 'and a small piece of chocolate .'],\n",
       " ['you can t go down on the price .', 'by how much ?'],\n",
       " ['not really . her eyes are blue .', 'can she speak chinese ?'],\n",
       " ['how much do we owe you ?', 'eight yuan twenty cents .'],\n",
       " ['that will be fine .', 'what else can i get for you ?'],\n",
       " ['seven and a half .', 'here we are . how does it fit ?'],\n",
       " ['i think there is an accident ahead .', 'we are now running late .'],\n",
       " ['do you have the time ?', 'it s ten thirty .'],\n",
       " ['i mean everyone will leave !', 'i mean come on that is just noise !'],\n",
       " ['do you know your credit score ?', 'i think that it is around .'],\n",
       " ['do you like travelling ?', 'yes i do .'],\n",
       " ['what a pity !', 'and a storm blew our roof away .'],\n",
       " ['how s the chicken ?', 'it s delicious .'],\n",
       " ['hello john . how are you ?', 'i m fine thanks . and you ?'],\n",
       " ['no i d like the fish .', 'whatever you say .'],\n",
       " ['thanks .', 'you are welcome .'],\n",
       " ['we re having trouble with bob .', 'what s the problem ?'],\n",
       " ['that s okay .', 'please put your name on this list .'],\n",
       " ['do you like it here so far ?', 'i am really enjoying it here so far .'],\n",
       " ['here you are .', 'all right . just a moment please .'],\n",
       " ['yeah like in a cab . . .', 'save it .'],\n",
       " ['oh no no this place is totally healthy !', 'that this milk is mine .'],\n",
       " ['my sister was taken to the hospital yesterday', 'what happened ?'],\n",
       " ['right !', 'and that s it . you re done .'],\n",
       " ['how about tomorrow afternoon ?', 'tomorrow afternoon would be fine .'],\n",
       " ['yes .here s my identification .', 'that s all you need .'],\n",
       " ['haven t seen you for a long time', 'i ve been away on a vacation .'],\n",
       " ['what time does the train get to london ?', 'at .'],\n",
       " ['nine days .', 'all right . thank you .'],\n",
       " ['so i can have five for five dollars ?',\n",
       "  'actually you can only have four .'],\n",
       " ['no it s last year s model .', 'true . how much did you pay ?'],\n",
       " ['ohh okay i m sorry .', 'you re right .'],\n",
       " ['welcome sir what can i do for you ?', 'i want to buy some records .'],\n",
       " ['sure . please come this way .', 'may i try it on ?'],\n",
       " ['sure . i m learning chinese through songs .',\n",
       "  'so you re learning chinese songs ?'],\n",
       " ['i m so glad you told me that .', 'good luck on your first election .'],\n",
       " ['you mean a backup ?', 'exactly !'],\n",
       " ['what time does it begin ?', 'at eight thirty .'],\n",
       " ['how long have you been married ?', 'about seven years now . and you ?'],\n",
       " ['is a white blouse ok ?', 'yeah fine . and dressing shoes .'],\n",
       " ['no kid . this is the gynecology department .', 'gynecology ?'],\n",
       " ['i think so hurry up and get off .', 'where are we ?'],\n",
       " ['could you weigh these please ?', '. please . anything else ?'],\n",
       " ['yes i ache all over .', 'are you coughing much ?'],\n",
       " ['which university did you graduate from ?',\n",
       "  'i graduated from peking university .'],\n",
       " ['what about water pollution instead of pollution ?', 'well done !'],\n",
       " ['i want to get a passport .', 'you need to apply for a passport .'],\n",
       " ['hi .', 'you mind if i'],\n",
       " ['you need to fill out an application form .', 'where is it ?'],\n",
       " ['help yourself .', 'super !'],\n",
       " ['have you seen that house ?', 'yes it looks interesting .'],\n",
       " ['when will our party be held ?', 'next wednesday .'],\n",
       " ['that s not a good excuse .', 'tell me what you eat in the morning .'],\n",
       " ['um .', 'and a company car .'],\n",
       " ['how about going to italy next summer vacation ?', 'why not ?'],\n",
       " ['i don t want to talk about it .', 'maybe i can help .'],\n",
       " ['how do you want to your eggs ?', 'fried please .'],\n",
       " ['it will be held at the music hall .', 'what s on the program ?'],\n",
       " ['i m not freaking out .', 'why would i be freaking out ?'],\n",
       " ['peter how often do you exercise ?', 'well i swim and run every day .'],\n",
       " ['fine . beginning when ?', 'two weeks from thursday for three nights .'],\n",
       " ['is he married ?', 'yes he s married .'],\n",
       " ['i major in japanese .', 'what do you think of the literature course .'],\n",
       " ['yes i have .', 'good . so which is your top choice ?'],\n",
       " ['out of sight out of mind .', 'who do you mean ?'],\n",
       " ['it s on the third floor .', 'thank you .'],\n",
       " ['no i ll have the fish please .', 'the chicken is also nice .'],\n",
       " ['what do they look like ?', 'white .'],\n",
       " ['i was robbed .', 'when did this happen ?'],\n",
       " ['what do you want for your drink ?', 'cola please .'],\n",
       " ['okay . don t forget to call susan .', 'don t worry . bye .'],\n",
       " ['dad it s raining outside .', 'that is your excuse .'],\n",
       " ['thank goodness .', 'please pay yuan the handling charge .'],\n",
       " ['what s the temperature today ?', 'it s about degrees centigrade .'],\n",
       " ['i ll need to see your id please .', 'here you go .'],\n",
       " ['i wasn t feeling really well .', 'what was wrong with you ?'],\n",
       " ['pack some necessary medicine in your carry on .',\n",
       "  'i almost forget it . thank you !'],\n",
       " ['all right ! i found one that fits !', 'well y know what they say the'],\n",
       " ['what can i do for you ?', 'i would like to cancel a check .'],\n",
       " ['ok i ll expected you around o clock ?', 'see you then .'],\n",
       " ['in whose name was the reservation made ?', 'jim white .'],\n",
       " ['thanks a lot', 'you are welcome'],\n",
       " ['thank you .', 'don t mention it .'],\n",
       " ['single or double ?', 'make it two .'],\n",
       " ['do you know the news ?', 'which one ?'],\n",
       " ['for how many people ?', 'only one .'],\n",
       " ['i cannot get on a plane without one !', 'i mean this is so cool !'],\n",
       " ['right here .', 'well it s getting late .'],\n",
       " ['could you lend me one ?', 'ok . here you are .'],\n",
       " ['okay i ll ask bob to cover it .', 'thanks . i appreciate it .'],\n",
       " ['i see .', 'but thank you very much all the same .'],\n",
       " ['well you did pull his hair .', 'he took my snack !'],\n",
       " ['oh oh professor geller .', 'ahh to be again .'],\n",
       " ['all right see you tonight .', 'see you .'],\n",
       " ['how are you doing ?', 'great . thanks for asking .'],\n",
       " ['no we re not .', 'yeah we are .'],\n",
       " ['you have a warrant .', 'a warrant for what ?'],\n",
       " ['you can take any bus except the number .', 'how often do the buses run ?'],\n",
       " ['no this isn t .', 'could you please get him on the phone ?'],\n",
       " ['will you lose all your files ?', 'no i always back up my files .'],\n",
       " ['well kinda old like .', 'oh .'],\n",
       " ['please take a seat .', 'thank you madam .'],\n",
       " ['hi bill this is my new car .', 'hey great set of wheels .'],\n",
       " ['that is exactly what we re looking for .',\n",
       "  'well each room is dollars a week .'],\n",
       " ['would you show me some table cloths ?',\n",
       "  'certainly . take a table cloth to her'],\n",
       " ['so what do you feel like eating then ?', 'how about some burgers ?'],\n",
       " ['i think that s too late .', 'is pm ok ?'],\n",
       " ['did you go to see michael last night ?',\n",
       "  'yes he had just bought a new motorcycle .'],\n",
       " ['i don t remember .', 'what was your speed then ?'],\n",
       " ['sounds good when ?', 'when is it convenient for you ?'],\n",
       " ['how long have you been like this ?', 'for two days .'],\n",
       " ['how nice .', 'i may take you there some day .'],\n",
       " ['is there anything i can do for you ?', 'i m planning a trip to europe .'],\n",
       " ['how long do you expect her to live ?', 'about half a year .'],\n",
       " ['thanks .', 'where is your birthplace ?'],\n",
       " ['hello i d like to open an account .',\n",
       "  'which account would you like to open ?'],\n",
       " ['very sweet .', 'how much are they ?'],\n",
       " ['we have italian french and thousand island .', 'make it french please .'],\n",
       " ['can you recommend some popular tour ?', 'how long is your journey ?'],\n",
       " ['yes i think so .', 'ok . please fill out this loan application .'],\n",
       " ['mary s is handling it .', 'is everything under control ?'],\n",
       " ['would that be for here or take away ?', 'that would be to go .'],\n",
       " ['i m going to pcc .', 'how do you like it so far ?'],\n",
       " ['tom your task is to clean the blackboard .', 'how about yourself ?'],\n",
       " ['not at all .', 'be careful of yourself .'],\n",
       " ['how about the seaside ?', 'anything you say honey .'],\n",
       " ['hello . can i speak to anne ?', 'who s calling ?'],\n",
       " ['yes but do you love me ?', 'why ? you mean a lot to me .'],\n",
       " ['me ? i m in the car business .', 'salesman ?'],\n",
       " ['my boyfriend really does have good taste !',\n",
       "  'thanks for picking out the earrings man .'],\n",
       " ['well who all is going to be there ?', 'everyone from school .'],\n",
       " ['beef please .', 'how about drinks ? coffee or tea ?'],\n",
       " ['thank you .', 'how do you keep fit ?'],\n",
       " ['i have a terrible toothache .', 'which tooth is it ?'],\n",
       " ['how about the next one ?', 'in an hour .'],\n",
       " ['two . french and spanish .', 'and how well can you speak them ?'],\n",
       " ['but she means a lot to me .', 'then forgive her mistake .'],\n",
       " ['no it s ?', 'oh i m so sorry .'],\n",
       " ['do you have anything to declare ?',\n",
       "  'no these are all my personal effects .'],\n",
       " ['he looks tired all the time .', 'he s working too hard .'],\n",
       " ['thank you very much for your kindness .', 'is there any clue ?'],\n",
       " ['pure gold ones please .', 'ok . here they are .'],\n",
       " ['hello ! is this mr . chang ?', 'yes . who is speaking ?'],\n",
       " ['i don t have another one .', 'what about cash ?'],\n",
       " ['thank you very much .', 'did you have a good trip ?'],\n",
       " ['premium or regular ?', 'regular please .'],\n",
       " ['yes what is it ?', 'some more chinese tea for us please .'],\n",
       " ['i see .', 'but thank you very much all the same .'],\n",
       " ['really ?', 'i wouldn t lie about something like that .'],\n",
       " ['i decided that you re right .', 'i m glad you saw the reason .'],\n",
       " ['i need some us dollars .', 'what kind of currency have you got ?'],\n",
       " ['it won t start .', 'do you want me to take a look ?'],\n",
       " ['hello what can i do for you ?', 'i am looking for a host home .'],\n",
       " ['do you charge for checking it out ?', 'no .'],\n",
       " ['very well i ll try the banana flavor .', 'anything else sir ?'],\n",
       " ['i m just looking .', 'then take your time please .'],\n",
       " ['no .', 'what about his hair ?'],\n",
       " ['that s a good idea .what s on ?', 'gone with the wind .'],\n",
       " ['oh it s almost eleven twenty .', 'thank you .'],\n",
       " ['here you are .it s yuan miss .', 'can you give me a discount ?'],\n",
       " ['how would you like to send it ?', 'by airmail .'],\n",
       " ['is there any furniture in it ?', 'yes it s fully furnished .'],\n",
       " ['i m going to miss you folks too .', 'let s keep in touch .'],\n",
       " ['i often feel so tired .', 'you better do some exercise every morning .'],\n",
       " ['what are you doing tonight ?', 'huh ? uh'],\n",
       " ['a visitor .', 'have a good time !'],\n",
       " ['i ve just been working really hard .', 'i ve also been busy .'],\n",
       " ['how many persons are there in your family ?',\n",
       "  'there are three my parents and i .'],\n",
       " ['nothing .', 'doesn t look like it to me .'],\n",
       " ['hmm . i have only one best friend .', 'i feel sorry for you .'],\n",
       " ['fine .', 'could you tell us your specials today ?'],\n",
       " ['english is my favorite .', 'no wonder your english is so good .'],\n",
       " ['guess !', 'it s not you is it ?'],\n",
       " ['i m having a problem .', 'what is it ?'],\n",
       " ['i think you ve lost it .', 'well that s my opinion .'],\n",
       " ['i want to go traveling .', 'do you have any where in mind ?'],\n",
       " ['that sounds interesting .', 'yes it helps me to relax .'],\n",
       " ['i have a question .', 'what do you need to know ?'],\n",
       " ['thank you you are very kind !', 'you are welcome !'],\n",
       " ['did i mess up on anything ?', 'you did mess up on something .'],\n",
       " ['i wish he could too .', 'how s your boy jack ?'],\n",
       " ['it s really nice .', 'thanks again . you look nice today too .'],\n",
       " ['it is our th wedding anniversary this june .',\n",
       "  'our th wedding anniversary ?'],\n",
       " ['are you coughing much ?', 'a little bit .'],\n",
       " ['thank you . goodbye .', 'goodbye . hope to see you again .'],\n",
       " ['may i help you ?', 'yes i am looking for a coat .'],\n",
       " ['morning or afternoon ?', 'afternoon please if that s possible .'],\n",
       " ['oh really ?', 'yes i am a volunteer for the campaign .'],\n",
       " ['take two tablets every six hours .',\n",
       "  'i see . i ll follow your instructions .'],\n",
       " ['i have a meeting this afternoon .', 'when will it begin ?'],\n",
       " ['how much would you like to exchange ?',\n",
       "  'what s the exchange rate for rib ?'],\n",
       " ['there is dial up or del .', 'which one do you feel is best ?'],\n",
       " ['which season do you like best ?', 'i like spring .'],\n",
       " ['merry christmas !', 'the same to you !'],\n",
       " ['in london .where do you live ?', 'here .near this school .'],\n",
       " ['this is soft may i speak to lamely ?', 'this is lamely .'],\n",
       " ['let s go get something to drink .', 'that s a good idea .'],\n",
       " ['nice to meet you .', 'do you have any experience using weights ?'],\n",
       " ['how long have you had it ?', 'it all started the day before yesterday .'],\n",
       " ['usually shanghai qingdao hong kong and so on .',\n",
       "  'it s great ! i like these places .'],\n",
       " ['yes .', 'what should i do ?'],\n",
       " ['do you take dressing on your salad ?', 'yes blue cheese please .'],\n",
       " ['i can t go to your party .', 'that s too bad .'],\n",
       " ['what is that ?', 'melbourne zoo .'],\n",
       " ['wow i can t thank you enough .', 'don t mention it .'],\n",
       " ['it s not what i asked for .', 'what s wrong with it ?'],\n",
       " ['please fill out the form first .', 'ok can you bring me a new one ?'],\n",
       " ['thank you for your interest in our company .',\n",
       "  'ok . thank you very much .'],\n",
       " ['please show me your receipt again .', 'here it is .'],\n",
       " ['also an island .', 'hey what time is it ?'],\n",
       " ['single or return ?', 'return please .'],\n",
       " ['yes . i have just booked two tickets .', 'tickets ? what tickets ?'],\n",
       " ['does it look too small ?', 'it fits you to a t .'],\n",
       " ['no i haven t been registered .', 'are you a medical or surgical case ?'],\n",
       " ['how come ?', 'because it helps you speed up your circulation .'],\n",
       " ['like your sweater .', 'oh hey right back at ya .'],\n",
       " ['do you have some chocolate cakes ?', 'yes we have .'],\n",
       " ['have you taken your temperature ?', 'yes and it is c .'],\n",
       " ['oh i didn t go to college .', 'no where did you study acting ?'],\n",
       " ['may i try it on ?', 'of course please .'],\n",
       " ['don t tell me you lost it .', 'ah umm kind of .'],\n",
       " ['ok i ve done that .', 'then just press this button .'],\n",
       " ['i have an issue .', 'what s going on ?'],\n",
       " ['well', 'yeah .'],\n",
       " ['no i don t .', 'is it urgent ?'],\n",
       " ['want to send out for some chinese ?', 'some what ?'],\n",
       " ['i take it you ve done this before ?', 'yes for the last years .'],\n",
       " ['no this is pretty much it .', 'you guys wanna get some coffee ?'],\n",
       " ['when do you celebrate thanksgiving ?',\n",
       "  'on the fourth thursday of november .'],\n",
       " ['sure . the fitting room is over there .', 'thank you .'],\n",
       " ['thank you helen that ll be all .', 'last time i do that i promise .'],\n",
       " ['yeah ?', 'are you phoebe buffay ?'],\n",
       " ['what about the jeans ?', 'they don t really fit you right .'],\n",
       " ['hi my sweetheart . it s dad .', 'hi dad . where are you ?'],\n",
       " ['what is it ?', 'twice cooked spicy pork slices .'],\n",
       " ['it s nice to finally meet you .', 'and i m glad to meet you too .'],\n",
       " ['happy new year !', 'thank you . same to you .'],\n",
       " ['how did you like it ?', 'it s fantastic .'],\n",
       " ['you don t like the performance do you ?',\n",
       "  'yes but i don t like the story .'],\n",
       " ['a day or a week unlimited mileage .',\n",
       "  'could i have one for tomorrow morning ?'],\n",
       " ['could i have some fish ?',\n",
       "  'certainly . and what vegetables would you like ?'],\n",
       " ['is there one in particular that you like ?',\n",
       "  'i was looking at this kenmore refrigerator .'],\n",
       " ['i think so .', 'let s step in dad .'],\n",
       " ['whoa ! oh no !', 'oops !'],\n",
       " ['may i help you ?', 'what time will the library be closing ?'],\n",
       " ['is the bus ride long ?', 'it only takes minutes to an hour .'],\n",
       " ['let me get this .', 'no it is my treat .'],\n",
       " ['just let it go .', 'how are your work now ?'],\n",
       " ['yes i ll be promoted to department manager .',\n",
       "  'i m glad to hear that . congratulations !'],\n",
       " ['the fare is . .', 'have you been driving buses a long time ?'],\n",
       " ['we re redecorating our living room .', 'what are you going to do to it ?'],\n",
       " ['i can t believe that .', 'what ?'],\n",
       " ['who s it about ?', 'it s about bob dylan .'],\n",
       " ['hi what can i get for you ?', 'hello may i have a double cheeseburger ?'],\n",
       " ['with sugar ?', 'no sugar please .'],\n",
       " ['yes . when ?', 'is am ok ?'],\n",
       " ['i suppose there s nothing cheaper is there ?',\n",
       "  'no . nothing . i m sorry .'],\n",
       " ['were you speeding when you got pulled over ?', 'i was not speeding .'],\n",
       " ['it thirty eight dollars .', 'all right . here s forty dollars .'],\n",
       " ['ok . that will be rmb please .', 'how long is the flight ?'],\n",
       " ['why do you have to leave early ?', 'i am not feeling well .'],\n",
       " ['you should really drink water .', 'that sounds good .'],\n",
       " ['thank you very much .', 'you re welcome .'],\n",
       " ['is it a non stop flight ?', 'yes .'],\n",
       " ['medium please .', 'anything else to go ?'],\n",
       " ['this is john .', 'what is it john ?'],\n",
       " ['i want a chicken salad .', 'i enjoy chicken salads .'],\n",
       " ['yes here is the form .', 'is this your luggage ?'],\n",
       " ['but he still does not come back .', 'maybe he is on the way home now .'],\n",
       " ['thank you .what s the fare ?', '.'],\n",
       " ['how old are you ?', 'twenty one .'],\n",
       " ['right away sir .', 'is this amount correct ?'],\n",
       " ['and your address please ?', 'lincoln avenue .'],\n",
       " ['what s in it ?', 'have a guess .'],\n",
       " ['i m looking for a black leather bag .', 'how s this ?'],\n",
       " ['hi steven .', 'how are you today ?'],\n",
       " ['oh hi . i was just watching tv .', 'there s nothing to watch right now .'],\n",
       " ['that is exactly what we re looking for .',\n",
       "  'well each room is dollars a week .'],\n",
       " ['you mustn t pick it in this place .', 'where it is ?'],\n",
       " ['. ?', 'sorry is the lowest i can go .'],\n",
       " ['good . how about you ?', 'not bad thanks for asking .'],\n",
       " ['we should do this more often .', 'yes we should .'],\n",
       " ['excuse me what s your name ?', 'my name is jessica . what s yours ?'],\n",
       " ['i m too careless .', 'can it work now ?'],\n",
       " ['i ve got a headache and sore throat .', 'how long have you had it ?'],\n",
       " ['give me a call tomorrow .', 'okay .'],\n",
       " ['we think the price is too high .', 'that s the best price we can offer .'],\n",
       " ['do you live with your family ?', 'yes we live a happy life together .'],\n",
       " ['all right i m i m not english .', 'i m from long island .'],\n",
       " ['are you serious ?', 'i was there .'],\n",
       " ['you get off on del mar and hill .', 'thanks for your help .'],\n",
       " ['english of course .', 'is she your teacher ?'],\n",
       " ['here is your bill . yuan in all .', 'do you accept check ?'],\n",
       " ['no more . thank you .', 'here s to our friendship and health !'],\n",
       " ['c mon . just don t move .', 'i m bleeding too much .'],\n",
       " ['ok thanks . . .', 'you re new at this huh ?'],\n",
       " ['okay thank you .', 'that ll be all .'],\n",
       " ['not much i have a car .', 'so you have your own car ?'],\n",
       " ['unfortunately not .', 'that s ok . is breakfast included ?'],\n",
       " ['that s fine .', 'please fill out this application .'],\n",
       " ['where s the wagon ?', 'where s what wagon ?'],\n",
       " ['yes he s really pushed me too far .', 'i know what you mean .'],\n",
       " ['give me a book please jane .', 'which book ? this one ?'],\n",
       " ['it s the felony charge .', 'i m willing to provide my services .'],\n",
       " ['your late fees come to . .', 'i don t have that right now .'],\n",
       " ['what time do you want me there ?', 'anytime after six will be fine .'],\n",
       " ['because i think it s boring', 'no you can learn a lot from them .'],\n",
       " ['oh nothing .', 'what were you writing ?'],\n",
       " ['it only takes minutes to an hour .', 'that s all ?'],\n",
       " ['no ! they are mine !', 'you stole them from me !'],\n",
       " ['toast dear ?', 'as you like .'],\n",
       " ['will you show me your key card please ?', 'here it is .'],\n",
       " ['i feel kind of stupid .', 'why ?'],\n",
       " ['look after yourself . bye .', 'bye .'],\n",
       " ['thanks . what time will you arrive ?', 'around pm .'],\n",
       " ['never .', 'never ?'],\n",
       " ['could you handle flying for hours straight ?',\n",
       "  'yes i can adjust to anything .'],\n",
       " ['we re redecorating our living room .', 'what are you going to do to it ?'],\n",
       " ['three twenty five cent stamps please .',\n",
       "  'here you are . seventy five cents please .'],\n",
       " ['i can get that for you .', 'thanks so much .'],\n",
       " ['me too .', 'maybe we can meet again sometime ?'],\n",
       " ['sounds good to me .', 'where can we go to get both ?'],\n",
       " ['can i see your driver s license ?', 'sure here is my driver s license .'],\n",
       " ['nice to meet you too .', 'you are a graduate right ?'],\n",
       " ['i will receive a bachelor s degree .', 'what is your major ?'],\n",
       " ['anyway umm so i was um i was hiking', 'i love hiking !'],\n",
       " ['yes it s me .', 'do you have a cold ?'],\n",
       " ['can you supply us right away ?', 'yes we have plenty on hand right now .'],\n",
       " ['yes i will . take care then .', 'yes sure . have a sweet dream !'],\n",
       " ['okay i ll do that .', 'thank you . bye bye !'],\n",
       " ['you can t beat me at tennis .', 'do you want to bet ?'],\n",
       " ['i should not have listened to you .',\n",
       "  'i really thought this was our stop .'],\n",
       " ['good .', 'so you owe me three pretty things .'],\n",
       " ['can i have your name please ?', 'my name is james .'],\n",
       " ['is there anything else ?', 'i d like a bottle of juice .'],\n",
       " ['sure here you are . it comes to .',\n",
       "  'does that include the service charge ?'],\n",
       " ['ahh .', 'do you love her ?'],\n",
       " ['is there anything i can do for you ?', 'i want to have a trip to sydney .'],\n",
       " ['so you didn t . is that right ?', 'yes . i m sorry .'],\n",
       " ['yes .', 'what s your name ?'],\n",
       " ['does he have a temperature doctor ?', 'no he doesn t .'],\n",
       " ['would you please mail these letters lucy ?', 'yes sir .'],\n",
       " ['i want to get some pizza .', 'i had pizza for lunch yesterday .'],\n",
       " ['sure .that would be great .good luck .', 'thank you .'],\n",
       " ['promise that you won t get angry .', 'ok i promise . what is it ?'],\n",
       " ['good morning . how can i help you ?', 'i d like to open a new account .'],\n",
       " ['i ve gotten chow mein from there before .',\n",
       "  'i guess we can eat there then .'],\n",
       " ['do you want any sugar ?', 'yes please .'],\n",
       " ['yes i can but not very well .',\n",
       "  'can you understand your spanish teacher ?'],\n",
       " ['when can i come over ?', 'would friday morning be alright ?'],\n",
       " ['i m jack . what s your name ?', 'laura .'],\n",
       " ['what do you know about julia roberts ?', 'her acting is ok .'],\n",
       " ['it s london .', 'and your name and number ?'],\n",
       " ['is that true ?', 'definitely !'],\n",
       " ['i m making a shopping list tom .', 'what do we need ?'],\n",
       " ['because we re girls .', 'phoebe his music could not'],\n",
       " ['you re right .', 'you re right .'],\n",
       " ['i d like to cash this check please .', 'do you have an account with us ?'],\n",
       " ['yes . i can lend you some .', 'thank you . but i m not interested .'],\n",
       " ['yes ?', 'i m afraid you can t smoke here .'],\n",
       " ['oh no . not at all .', 'are you planning on studying abroad ?'],\n",
       " ['i m sure you re ready .', 'does it cost money ?'],\n",
       " ['for today ?', 'no early saturday morning .'],\n",
       " ['good bye then and keep in touch .', 'good bye .'],\n",
       " ['rachel ! rachel !', 'what ?'],\n",
       " ['may i take your order ?', 'we d like this course for two please .'],\n",
       " ['may i try on this pair of shoes ?', 'of course . what is your size ?'],\n",
       " ['so see you .', 'bye .'],\n",
       " ['would you rather stay home ?', 'whatever you say .'],\n",
       " ['anything else you ll be needing today sir ?',\n",
       "  'nope . that s everything thanks .'],\n",
       " ['y know we haven t found anybody else .', 'it might be kinda cool .'],\n",
       " ['of course . they are very friendly .', 'which floor are you on ?'],\n",
       " ['see you around !', 'goodbye alice .'],\n",
       " ['ok that s fine . bye .', 'what happened ?'],\n",
       " ['what are you buying ?', 'i don t know what we need .'],\n",
       " ['who s that old lady trimming the trees ?', 'she s my grandma .'],\n",
       " ['that s cool .', 'yeah i know .'],\n",
       " ['yes . very wonderful .', 'there is a snowman over there .'],\n",
       " ['thank you .', 'the rent is a month ?'],\n",
       " ['it s just what i want .', 'no regrets .'],\n",
       " ['how long are you going to stay ?', 'for one week .'],\n",
       " ['there s a letter here for you .', 'let me see it .'],\n",
       " ['you re dancing so well .', 'you dance beautifully too .'],\n",
       " ['i got it .', 'i got it .'],\n",
       " ['did you hear the good news ?', 'no i haven t .'],\n",
       " ['what s the movie about ?', 'i m not sure .'],\n",
       " ['should i ask sara to the party ?', 'i would if i were you .'],\n",
       " ['may just let him go .', 'but he took some newspaper away without paying .'],\n",
       " ['well it s only three stops from here .', 'i see . thanks a lot .'],\n",
       " ['what do you think of it ?', 'beautifully done many thanks .'],\n",
       " ['when did the trouble start ?', 'i was sick most of the night .'],\n",
       " ['yes . do you mind ?', 'you really have to ?'],\n",
       " ['will you look at this form ?', 'are you having problems with it ?'],\n",
       " ['ok .thanks .', 'your bags will be here shortly .'],\n",
       " ['i ve actually been busy lately .', 'what have you been doing ?'],\n",
       " ['i m looking for a sweater .', 'what size are you looking for ?'],\n",
       " ['it s your turn now helen .', 'ok . i m coming .'],\n",
       " ['which train do you like ?', 't to washington please .'],\n",
       " ['oh yes ?', 'it s true sir .'],\n",
       " ['in what room is this pipe ?', 'you ll find the pipe in my bathroom .'],\n",
       " ['how bad is this ?', '!'],\n",
       " ['yes thank you very much .', 'you are welcome .'],\n",
       " ['is that room service ?', 'yes .what can i do for you ?'],\n",
       " ['is that all ?', 'yes . that s the minimum .'],\n",
       " ['so you have seen her around ?', 'yes i have .'],\n",
       " ['well congratulations . when is the big date ?', 'in june .'],\n",
       " ['lucy green .', 'you are booked ms . green .'],\n",
       " ['hold on .', 'hold on .'],\n",
       " ['okay .', 'so ask me what i did today .'],\n",
       " ['not too good i m sorry .', 'i lost ?'],\n",
       " ['where are you going tony ?', 'it s none of your business .'],\n",
       " ['so is everybody here ?', 'i got here a little early myself .'],\n",
       " ['yeah . it s already december you know .', 'time sure flies doesn t it ?'],\n",
       " ['i see .', 'but thank you very much all the same .'],\n",
       " ['yeah absolutely !', 'yeah .'],\n",
       " ['i had to mail in an absentee ballot .', 'why d you have to do that ?'],\n",
       " ['you sure sound like naomi .', 'oh . can i take a message ?'],\n",
       " ['i d rather be cold than hot .', 'me too .'],\n",
       " ['hi b . how are you ?', 'good .'],\n",
       " ['he s a famous american musician .', 'who s the author ?'],\n",
       " ['it s a lovely day isn t it ?', 'yes the weather sure is nice today .'],\n",
       " ['could you do me a favor ?', 'sure whatever you need .'],\n",
       " ['good idea .', 'who will sing in the show ?'],\n",
       " ['she wants to be a manager .', 'what classes does she take ?'],\n",
       " ['no you can t do that .', 'sure i can my treat .'],\n",
       " ['what will you do this vacation ?', 'travel .'],\n",
       " ['we offer hour service .', 'oh that s great ! thank you .'],\n",
       " ['will that be all ?', 'that s everything .'],\n",
       " ['how much is it ?', 'this one sells for . .'],\n",
       " ['what s the fare ?', 'per hour .'],\n",
       " ['may i have the menu please ?', 'yes here you go .'],\n",
       " ['hi is that mr . wu ?', 'yes . what can i do for you ?'],\n",
       " ['we ate everything from dinner .', 'i just need a snack .'],\n",
       " ['for what ?', 'the test tomorrow .'],\n",
       " ['do you have your insurance card with you ?', 'no i don t .'],\n",
       " ['excuse me do you speak english ?', 'yes do you need some help ?'],\n",
       " ['that s out of the question .', 'please .'],\n",
       " ['ten yuan .', 'and how much is this pencil ?'],\n",
       " ['nick ! how s it going ?', 'oh hey . . .'],\n",
       " ['yes it is . kelly ?', 'yes it s me .'],\n",
       " ['let s say about ?', 'that s ok with me .'],\n",
       " ['that s lucky .', 'why is that ?'],\n",
       " ['i apologize for their tardiness .',\n",
       "  'i was late for work because of them .'],\n",
       " ['why not ?', 'let s get a soft drink .'],\n",
       " ['tomorrow night would work for me .', 'should we just meet here ?'],\n",
       " ['it only costs you dollars for one .', 'ok i will take one .'],\n",
       " ['ok . here you are .', 'thank you . do you need the invoice ?'],\n",
       " ['may i introduce myself ?', 'yes .'],\n",
       " ['yes he s in his office .', 'oh i hope he didn t notice me .'],\n",
       " ['what for ? for the party of course .', 'party ? what party ?'],\n",
       " ['you look really beautiful in it .', 'ok . i ll take it .'],\n",
       " ['is everything alright ?', 'yes everything is fine .'],\n",
       " ['no it s at counter right over there .',\n",
       "  'at counter . do you take parcels here ?'],\n",
       " ['my name is paul .is mr .smith in ?', 'yes he is .'],\n",
       " ['i want it transferred into my checking account .',\n",
       "  'how much would you like to transfer ?'],\n",
       " ['fine thanks .and you ?', 'very well thanks .'],\n",
       " ['umm can i use your bathroom ?', 'it s uh right through there .'],\n",
       " ['what kind of room would you like ?', 'double rooms with twin beds .'],\n",
       " ['in seven days it will be christmas day .', 'yes i know that .'],\n",
       " ['every five minutes .', 'oh here comes a bus !'],\n",
       " ['oh would you do me a favor ?', 'yes ?'],\n",
       " ['what s her last name ?', 'it s snow .'],\n",
       " ['it s blond .', 'and how old is she ?'],\n",
       " ['i m very fond of you .', 'yes but do you love me ?'],\n",
       " ['zhang z h a n g .', 'what is your present address ?'],\n",
       " ['yeah yeah i was watching .', 'umm hey a couple of questions though .'],\n",
       " ['how are your work now ?', 'not bad just let it go .'],\n",
       " ['shave also sir .', 'yes .'],\n",
       " ['that s everything .', 'i need your library card .'],\n",
       " ['are you sure you heard right ?', 'yes he is running for class president .'],\n",
       " ['whole foods has a lot of organic foods ?',\n",
       "  'yeah the food there is very healthy .'],\n",
       " ['well do you have through tickets ?', 'yes sir it s .'],\n",
       " ['three times a day after meals .', 'thank you doctor .goodbye .'],\n",
       " ['it s about their great love .', 'sounds interesting .'],\n",
       " ['what is your father doing now ?', 'reading newspaper on the couch .'],\n",
       " ['how would you like your hair cut ?', 'i want it short .'],\n",
       " ['small please .', 'here you are .'],\n",
       " ['then we can put you in bed okay ?', 'just smile and don t talk to'],\n",
       " ['you have to help to the laundry today .', 'give me a break .'],\n",
       " ['i would like to book a table for .', 'ok sir . when will you be there ?'],\n",
       " ['hey !', 'ooh !'],\n",
       " ['what do you mean ?', 'eggs milk bread . things like that .'],\n",
       " ['that s a deal .', 'sure .'],\n",
       " ['yes . what is the rent ?', 'per month .'],\n",
       " ['please come in steven .', 'all right mr . green .'],\n",
       " ['hi .', 'hey !'],\n",
       " ['no .', 'hey waiter .'],\n",
       " ['where ?', 'at the gate .'],\n",
       " ['excuse me !', 'may i help you ?'],\n",
       " ['after all he is a baby .', 'yeah . anyway he is driving me mad .'],\n",
       " ['may i help you sir ?', 'yes i d like to get some gas .'],\n",
       " ['can i give her a message for you ?', 'yes please tell her i stopped by .'],\n",
       " ['please hurry .', 'here it is .'],\n",
       " ['umm thank you for meeting with me .', 'thank you . all right .'],\n",
       " ['is there a mirror around here ?', 'right over there .'],\n",
       " ['use the stairs never the elevator .', 'oh dear .'],\n",
       " ['okay come on .', 'now we can go eat .'],\n",
       " ['what kind do you have ?', 'we have italian french and thousand island .'],\n",
       " ['no sir . the next stop is wangfujing .', 'ok .'],\n",
       " ['all right thank you so much good bye .', 'good bye .'],\n",
       " ['thank you very much . take the pills .', 'are you feeling better now ?'],\n",
       " ['good afternoon how can i help you ?',\n",
       "  'someone has stolen my gold necklace .'],\n",
       " ['ok . goodbye .', 'see you later .'],\n",
       " ['pale yellow .', 'what else are you going to do ?'],\n",
       " ['wow . that sounds really good .', 'mm it is .'],\n",
       " ['what s up ?', 'could i ask you some questions ?'],\n",
       " ['good morning can i have your ticket please ?', 'here you are .'],\n",
       " ['maybe you re right .', 'we ll go tomorrow .'],\n",
       " ['what s the difference in price ?', 'yuan by air yuan by sea .'],\n",
       " ['oh dear !', 'what s the matter ?'],\n",
       " ['let s keep in touch .', 'sure let s . goodbye then .'],\n",
       " ['oh are the connections correct ?', 'i think so .'],\n",
       " ['yes .', 'have you heard what happened to him ?'],\n",
       " ['how old is she ?', 'about . she is a good girl .'],\n",
       " ['how would you like your bills ?', 'in fifties please .'],\n",
       " ['i wash and you ll dry .', 'ok . let s do it .'],\n",
       " ['how did you do on your driving test ?', 'passed .'],\n",
       " ['we are short of staff today .',\n",
       "  'there is something wrong with this dish .'],\n",
       " ['my hard drive crashed .', 'oh no . that s bad news .'],\n",
       " ['about my boyfriend .', 'again !'],\n",
       " ['it sounds like it will be .', 'i really hope it doesn t get cold .'],\n",
       " ['well i am telling you now .', 'yes but you might have told me before .'],\n",
       " ['you dance beautifully too .', 'when did you learn to dance ?'],\n",
       " ['ok i ll take a message .', 'thank you .'],\n",
       " ['my feelings exactly .', 'i really enjoy the beach in the summer .'],\n",
       " ['bye . take care .', 'you too .'],\n",
       " ['yes i was there as an english teacher .', 'did you enjoy yourself there ?'],\n",
       " ['but that s okay no problem .', 'no problem .'],\n",
       " ['good for you .', 'do you plan on voting ?'],\n",
       " ['well please sing one that s not sad .',\n",
       "  'ok i ll sing san francisco then .'],\n",
       " ['do you have anything to declare ?',\n",
       "  'no these are all my personal effects .'],\n",
       " ['may i invite you for a dance ?', 'with pleasure .'],\n",
       " ['it s ok good night .', 'good night .'],\n",
       " ['sure which beach are we going to ?', 'i wanted to go to malibu beach .'],\n",
       " ['who s singing in your home jim ?', 'it s ellie and her friends .'],\n",
       " ['fine sir .', 'that sounds great . two tsingtao beers please .'],\n",
       " ['you can get del or dial up .', 'which of those two is best ?'],\n",
       " ['i don t know .', 'did you call the repairman ?'],\n",
       " ['yes sunday sounds fine . what time ?', 'does six thirty suit you ?'],\n",
       " ['yes . may i ask who s calling ?', 'this is his friend greg .'],\n",
       " ['what was that ?', 'relax it was just a goat !'],\n",
       " ['good afternoon mary . how s business ?', 'not bad thanks .'],\n",
       " ['are they all right ?', 'the news said they should be fine .'],\n",
       " ['come on in . make yourself at home .', 'thanks a lot .'],\n",
       " ['they don t really fit you right .', 'i don t think so either .'],\n",
       " ['yes that s right .', 'how long will you be staying ?'],\n",
       " ['thank you .', 'how do you think of the weather today ?'],\n",
       " ['you re talking to her .', 'i ve called you a hundred times today .'],\n",
       " ['thank you !', 'bye !'],\n",
       " ['i signed up for it online .', 'that s really cool .'],\n",
       " ['how about eight o clock tomorrow morning ?',\n",
       "  'all right . see you tomorrow .'],\n",
       " ['phoebe ?', 'yeah ?'],\n",
       " ['that is so sweet !', 'yeah .'],\n",
       " ['yes please bring us two steaks .', 'sure . anything else ?'],\n",
       " ['hi is this rich ?', 'yeah who s calling ?'],\n",
       " ['mr . yes ?', 'i m not feeling too well .'],\n",
       " ['i took something earlier .', 'get better .'],\n",
       " ['did you get your bus pass ?', 'i haven t gone to get it yet .'],\n",
       " ['chinese food sounds good .', 'where are you going to get it from ?'],\n",
       " ['positive .', 'you don t think you could be wrong ?'],\n",
       " ['it s my favorite .', 'what other kind of food do you like ?'],\n",
       " ['ten .', 'western food or chinese food ?'],\n",
       " ['good morning passport please .', 'here you are .'],\n",
       " ['what can i do for you ?', 'i would like to file a complaint .'],\n",
       " ['i m so happy about that .', 'did you vote for him ?'],\n",
       " ['yes .', 'for here or to go ?'],\n",
       " ['sorry it had been sold out .', 'what a pity !'],\n",
       " ['yes i have been here since .', 'then why do you want to sell it ?'],\n",
       " ['what do you mean too late ?', 'by that time you will be fired .'],\n",
       " ['yes i found it again .', 'when ?'],\n",
       " ['and your name please ?', 'john anderson .'],\n",
       " ['we ruined everything .', 'ugh'],\n",
       " ['well . i really do have to go .', 'good night then .'],\n",
       " ['hello !', 'oh hi . it s mark .'],\n",
       " ['oh i m sorry to hear that .', 'thank you . can i cancel my reservation ?'],\n",
       " ['no this is her sister nancy .', 'you sure sound like naomi .'],\n",
       " ['y know hey i understand .', 'y know ?'],\n",
       " ['and is that all ?', 'yes this is everything on the list .'],\n",
       " ['yes i ll talk to her .', 'all right .'],\n",
       " ['what are you majoring in ?', 'i major in japanese .'],\n",
       " ['i did that .', 'what came up ?'],\n",
       " ['what s special for today ?', 'sweet and sour pro chops madam .'],\n",
       " ['i m going to quit the job .', 'why ?'],\n",
       " ['yes do you have an appointment ?', 'no i don t .'],\n",
       " ['yes i have my ticket with me .', 'let me see your id .'],\n",
       " ['excellent . how many would you like ?', 'two thousand would be fine .'],\n",
       " ['will you ever get another copy ?',\n",
       "  'we will definitely be getting another .'],\n",
       " ['to the beach', 'have you made your hotel reservation yet ?'],\n",
       " ['no i like the fish .', 'what ever you say ?'],\n",
       " ['thanks for your help .', 'no problem .'],\n",
       " ['first of all did you vote today ?', 'i already voted .'],\n",
       " ['no officer not at all .', 'then how can you explain your behavior ?'],\n",
       " ['do you really have to work today ?', 'yes . i m afraid so .'],\n",
       " ['have you got any wine or spirits ?', 'no i don t drink .'],\n",
       " ['next week .', 'ok . there is one on wednesday .'],\n",
       " ['what day is it on earth ?', 'it s an anniversary of our weeding .'],\n",
       " ['does this bus really go to the mall ?', 'it goes all the way there .'],\n",
       " ['what s the matter with it ?', 'i can t control the temperature .'],\n",
       " ['is there something wrong with your meter ?', 'no . i am sure about that .'],\n",
       " ['i have a big date tonight .', 'you have got a car haven t you ?'],\n",
       " ['don t use office phones for personal matters .', 'i got it .'],\n",
       " ['are you busy now ?', 'no . why ?'],\n",
       " ['and you can see me !', 'same as yesterday same as the day before .'],\n",
       " ['you have to go and fix it .', 'yes . i will .'],\n",
       " ['hi how re you doing ?', 'terrible .'],\n",
       " ['i don t think so .', 'why is that ?'],\n",
       " ['excuse me where can i find a guide ?',\n",
       "  'you can find one at the ticket office .'],\n",
       " ['just get out of my face !', 'woo easy .'],\n",
       " ['okay', 'i will i will find him .'],\n",
       " ['are you there yet ?', 'yes .'],\n",
       " ['no thanks . i have had enough .', 'what about some drink ?'],\n",
       " ['yes but do you love me ?', 'why ? you mean a lot to me .'],\n",
       " ['no problem . good luck .', 'yes i will need it . thanks .'],\n",
       " ['i d love to .', 'let s say about ?'],\n",
       " ['i ll certainly look into that .', 'thank you . goodbye .'],\n",
       " ['how are you doing today ?', 'i m fine .'],\n",
       " ['the big one .', 'yes .'],\n",
       " ['just sign here please .', 'sure . here you go .'],\n",
       " ['that is a good idea .', 'you ll make more money that way .'],\n",
       " ['how much do you want to pay ?', 'about a month .'],\n",
       " ['just get over here !', 'come in ! quickly !'],\n",
       " ['that s because it s time .', 'time for what ?'],\n",
       " ['yes i do .', 'do you usually travel by bus ?'],\n",
       " ['right here ! it hurts right here !', 'how long has it felt like this ?'],\n",
       " ['shh .', 'shh .'],\n",
       " ['no it s ?', 'oh i m so sorry .'],\n",
       " ['yes we have plenty on hand right now .', 'good we ll take suits .'],\n",
       " ['how much was the ticket ?', 'i only paid two dollars for mine .'],\n",
       " ['i m fond of dancing .', 'good .you really know how to enjoy yourself .'],\n",
       " ['okay !', 'hi joey !'],\n",
       " ['welcome ! come in please !', 'yes .'],\n",
       " ['is this your first year in college ?',\n",
       "  'i transferred from another school .'],\n",
       " ['oh you didn t have to . . .', 'it was nothing .'],\n",
       " ['i ll take this blouse and skirt please .', 'how would you like to pay ?'],\n",
       " ['i want him to have his uncle .', 'is my baby gonna have his uncle joey ?'],\n",
       " ['how can i get my driver s license ?',\n",
       "  'have you taken your driving test yet ?'],\n",
       " ['what kind of apples do you prefer ?', 'i ve always liked green apples .'],\n",
       " ['are you going to this class this morning ?', 'of course easy .no sweat .'],\n",
       " ['great ! thank you !', 'my pleasure and have a good time !'],\n",
       " ['sure . what time ?', 'the film will start at'],\n",
       " ['a bunch of people went looting last night .', 'they what ?'],\n",
       " ['and i couldn t find another phone .', 'that s okay .'],\n",
       " ['what s your flight time ?', 'at .'],\n",
       " ['check this out . can i have this ?', 'what ?'],\n",
       " ['will that be it ?', 'yes that s all .'],\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post(joy):did you call me ?\n"
     ]
    }
   ],
   "source": [
    "diversity_penality = True\n",
    "emotions = 1\n",
    "sentence = 'did you call me ?'\n",
    "print('Post({}):{}'.format(emo_dict[emotions],sentence))\n",
    "emotions = int(emotions)\n",
    "emotions = torch.LongTensor([emotions])\n",
    "### Format input sentence as a batch\n",
    "# words -> indexes\n",
    "indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "# Create lengths tensor\n",
    "lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "# Transpose dimensions of batch to match models' expectations\n",
    "input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "# Use appropriate device\n",
    "input_batch = input_batch.to(device)\n",
    "lengths = lengths.to(device)\n",
    "emotions = emotions.to(device)\n",
    "# Forward input through encoder model\n",
    "encoder_outputs, encoder_hidden = encoder(input_batch, lengths)\n",
    "# Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "# Initialize decoder input with SOS_token\n",
    "decoder_input = torch.ones((1,1), device=device, dtype=torch.long) * SOS_token\n",
    "# Set initial context value,last_rnn_output, internal_memory\n",
    "context_input = torch.zeros(1,hidden_size,dtype=torch.float)\n",
    "context_input = context_input.to(decoder.device)\n",
    "rnn_output = None\n",
    "\n",
    "node = BeamSearchNode(hiddenstate=decoder_hidden,decoder_input=decoder_input,\n",
    "                       context_input=context_input,emotions_emb=emotions,\n",
    "                       length=1,logProb=0,last_rnn_output = rnn_output,\n",
    "                       previousNode=None,g=g\n",
    "                      )\n",
    "sent_leng = 0\n",
    "# beam search\n",
    "K = 100\n",
    "# Iteratively decode one word token at a time\n",
    "# Forward pass through decoder\n",
    "nodes = PriorityQueue(maxsize=K)\n",
    "nodes.put((0,node))\n",
    "# diversity rate\n",
    "gamma = 2\n",
    "\n",
    "choices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diversity_penality = False\n",
    "score,node = nodes.get()\n",
    "#print('Last word at position {}'.format(node.leng))\n",
    "decoder_output, decoder_hidden,emotions,context_input,rnn_output,g = decoder(\n",
    "    node.decoder_input,node.emotions, node.hidden_state,\n",
    "    node.context_input,encoder_outputs,node.rnn_output\n",
    ")\n",
    "# Obtain most likely word token and its softmax score\n",
    "#decoder_output = decoder_output.unsqueeze(0)\n",
    "decoder_scores, decoder_input = torch.topk(decoder_output,k= K, dim=1)\n",
    "decoder_scores = torch.log(decoder_scores)\n",
    "if diversity_penality:\n",
    "    # apply based on rank\n",
    "    penalties = torch.arange(0,K,dtype=torch.float,device=device) * gamma\n",
    "    # apply penalties on the output\n",
    "    decoder_scores = decoder_scores - penalties\n",
    "token_choices = [decoder_input[0,i].item() for i in range(K)] \n",
    "token_scores = [decoder_scores[0,i].item() for i in range(K)] \n",
    "# for each candidate token, compute loss\n",
    "choices=[]\n",
    "for token,decoder_score in zip(token_choices,token_scores):\n",
    "    next_decoder_input = torch.ones((1,1),dtype=torch.long,device=device) * token\n",
    "    next_node = BeamSearchNode(decoder_hidden,node,next_decoder_input,\n",
    "                          decoder_score,node.leng + 1,emotions,rnn_output,context_input,g)\n",
    "    #print('This is {} words'.format(next_node.leng))\n",
    "    current_score = score - next_node.eval()\n",
    "    choices.append((current_score,next_node))\n",
    "\n",
    "choices = sorted(choices,key=lambda x:x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes.put(choices[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scroe,node = nodes.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 night 19.738767440171603\n",
      "1 EOS 19.75673692670213\n",
      "2 . 19.865372966856565\n",
      "3 ! 20.116601978741976\n",
      "4 to 20.12763871378988\n",
      "5 ? 20.151815444170467\n",
      "6 of 20.2693634162695\n",
      "7 out 20.273720617359515\n",
      "8 go 20.31004606688208\n",
      "9 time 20.346591678901092\n",
      "10 the 20.37854575846011\n",
      "11 you 20.385546816855882\n",
      "12 well 20.389900748205854\n",
      "13 not 20.443488517824452\n",
      "14 new 20.454484926078454\n",
      "15 clothes 20.48572211513232\n",
      "16 tomorrow 20.55312072174976\n",
      "17 a 20.610585313149908\n",
      "18 day 20.657905263578115\n",
      "19 tell 20.668125926006134\n",
      "20 me 20.70366445809089\n",
      "21 later 20.740371922242197\n",
      "22 london 20.824861187603243\n",
      "23 make 20.827371258045446\n",
      "24 bag 20.842260019346213\n",
      "25 my 20.875089571802622\n",
      "26 home 20.881644855638754\n",
      "27 for 20.890908029189884\n",
      "28 her 20.895599288721293\n",
      "29 t 20.962439587225834\n",
      "30 before 20.97471855101299\n",
      "31 in 20.977892651162925\n",
      "32 so 20.98159399689532\n",
      "33 motorcycle 20.98760446029624\n",
      "34 here 20.989850362990534\n",
      "35 college 21.002725509379463\n",
      "36 be 21.006921403293862\n",
      "37 check 21.031232737973017\n",
      "38 again 21.04824424175967\n",
      "39 i 21.05847444092949\n",
      "40 drag 21.060722795928818\n",
      "41 his 21.07340775243839\n",
      "42 were 21.077850103008775\n",
      "43 when 21.08136766210274\n",
      "44 am 21.083667379268626\n",
      "45 call 21.093167200255607\n",
      "46 had 21.10142247643747\n",
      "47 nice 21.119644328997925\n",
      "48 looking 21.14723875516189\n",
      "49 room 21.14746954431349\n",
      "50 know 21.174901982105997\n",
      "51 could 21.19189196010401\n",
      "52 records 21.200669032301594\n",
      "53 ok 21.243162982660646\n",
      "54 okay 21.246214740037182\n",
      "55 going 21.27055100648419\n",
      "56 bill 21.278057512195456\n",
      "57 all 21.282656537809725\n",
      "58 this 21.290108956571057\n",
      "59 help 21.296042989798394\n",
      "60 us 21.301764353683552\n",
      "61 another 21.30414663178561\n",
      "62 is 21.31031894731882\n",
      "63 game 21.33913720993428\n",
      "64 left 21.34034279033718\n",
      "65 smith 21.352015762302422\n",
      "66 seen 21.362170076255335\n",
      "67 watch 21.36282647656963\n",
      "68 monday 21.368147433538287\n",
      "69 get 21.372059813742837\n",
      "70 some 21.373257083556453\n",
      "71 coming 21.38785633662396\n",
      "72 on 21.395832050031864\n",
      "73 talk 21.399341707254045\n",
      "74 weeks 21.406327915580444\n",
      "75 club 21.41538700285702\n",
      "76 like 21.43687845922459\n",
      "77 returning 21.437631453109418\n",
      "78 doing 21.439532670707166\n",
      "79 good 21.44151822035027\n",
      "80 look 21.444370251105646\n",
      "81 plan 21.462087746263958\n",
      "82 by 21.472580342072494\n",
      "83 it 21.487586541536594\n",
      "84 rather 21.50224178890239\n",
      "85 cook 21.505168070004647\n",
      "86 with 21.518136267745728\n",
      "87 pair 21.524623976954235\n",
      "88 history 21.534455812795084\n",
      "89 money 21.535025292519812\n",
      "90 there 21.539974725275712\n",
      "91 weather 21.544759444876778\n",
      "92 buy 21.550660235746978\n",
      "93 just 21.552084207537135\n",
      "94 them 21.556160074744\n",
      "95 about 21.55770802417758\n",
      "96 number 21.58865774858571\n",
      "97 morning 21.597366156242323\n",
      "98 definitely 21.6022198128627\n",
      "99 have 21.603981112834298\n"
     ]
    }
   ],
   "source": [
    "for idx,each in enumerate(choices):\n",
    "    print(idx,voc.index2word[each[1].decoder_input.item()],each[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T00:20:45.603674Z",
     "start_time": "2019-04-03T00:20:12.276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> how are you doing ?\n",
      "Bot(neutral): i . for are . you you questions\n",
      "Bot(joy): i . for are . you you questions\n",
      "Bot(anger): i going . are . you you questions\n",
      "Bot(sadness): i going . are . you you questions\n",
      "Bot(fear): i going . are . you you questions\n",
      "> what are you talking about ?\n",
      "Bot(neutral): i have before am be work before\n",
      "Bot(joy): i have before am be work before\n",
      "Bot(anger): i have before am be work before\n",
      "Bot(sadness): i have before am be work before\n",
      "Bot(fear): i have before am be work before\n",
      "> where are you from ?\n",
      "Bot(neutral): i think going . house\n",
      "Bot(joy): i think going . house\n",
      "Bot(anger): i think going . house\n",
      "Bot(sadness): i think going . house\n",
      "Bot(fear): i think going . house\n",
      "> what do you like ?\n",
      "Bot(neutral): yes would would some some .\n",
      "Bot(joy): yes would would some some .\n",
      "Bot(anger): yes would would some some .\n",
      "Bot(sadness): yes would would some some .\n",
      "Bot(fear): yes would would some some .\n",
      "> what s meaning of life ?\n",
      "Error: Encountered unknown word.\n",
      "> what s mean of life ?\n",
      "Bot(neutral): about ! what a\n",
      "Bot(joy): about ! what a\n",
      "Bot(anger): about ! what a\n",
      "Bot(sadness): about ! about a\n",
      "Bot(fear): about ! what a\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6f88e3d882fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msearcher2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-d7e14d615d46>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc, emotion_dict, beam_search)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher2 = BeamSearchDecoder(encoder,decoder,voc.num_words)\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc,emo_dict,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
